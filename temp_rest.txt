class MechanismPartExtractor:
    """Extract Entity-Activity pairs for mechanism parts"""
    
    def __init__(self, config: ConfigLoader, nlp_model: spacy.Language) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.nlp = nlp_model
        self.entity_aliases = config.get('entity_aliases', {})
    
    def extract_entity_activity(self, text: str) -> Optional[EntityActivity]:
        """Extract Entity-Activity tuple from text"""
        doc = self.nlp(text)
        
        # Find main verb (activity)
        main_verb = None
        for token in doc:
            if token.pos_ == 'VERB' and token.dep_ in ['ROOT', 'ccomp']:
                main_verb = token
                break
        
        if not main_verb:
            return None
        
        # Find subject entity
        entity = None
        for child in main_verb.children:
            if child.dep_ in ['nsubj', 'nsubjpass']:
                entity = self._normalize_entity(child.text)
                break
        
        if not entity:
            # Try to find entity from NER
            for ent in doc.ents:
                if ent.label_ in ['ORG', 'PER']:
                    entity = self._normalize_entity(ent.text)
                    break
        
        if entity and main_verb:
            return EntityActivity(
                entity=entity,
                activity=main_verb.text,
                verb_lemma=main_verb.lemma_,
                confidence=0.85
            )
        
        return None
    
    def _normalize_entity(self, entity: str) -> str:
        """Normalize entity name using aliases"""
        entity_upper = entity.upper().strip()
        return self.entity_aliases.get(entity_upper, entity)


class FinancialAuditor:
    """Financial traceability and auditing"""
    
    def __init__(self, config: ConfigLoader) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.financial_data: Dict[str, Dict[str, float]] = {}
        self.unit_costs: Dict[str, float] = {}
        self.successful_parses = 0
        self.failed_parses = 0
        self.d3_q3_analysis: Dict[str, Any] = {}  # Harmonic Front 3 - D3-Q3 metrics
    
    def trace_financial_allocation(self, tables: List[pd.DataFrame], 
                                  nodes: Dict[str, MetaNode],
                                  graph: Optional[nx.DiGraph] = None) -> Dict[str, float]:
        """Trace financial allocations to programs/goals
        
        Harmonic Front 3 - Enhancement 5: Single-Case Counterfactual Budget Check
        Incorporates logic from single-case counterfactuals to test minimal sufficiency.
        For D3-Q3 (Traceability/Resources): checks if resource X (BPIN code) were removed,
        would the mechanism (Product) still execute? Only boosts budget traceability score
        if allocation is tied to a specific project.
        """
        for i, table in enumerate(tables):
            try:
                self.logger.info(f"Procesando tabla financiera {i+1}/{len(tables)}")
                self._process_financial_table(table, nodes)
                self.successful_parses += 1
            except Exception as e:
                self.logger.error(f"Error procesando tabla financiera {i+1}: {e}")
                self.failed_parses += 1
                continue
        
        # HARMONIC FRONT 3 - Enhancement 5: Counterfactual sufficiency check
        if graph is not None:
            self._perform_counterfactual_budget_check(nodes, graph)
        
        self.logger.info(f"Asignaciones financieras trazadas: {len(self.financial_data)}")
        self.logger.info(f"Tablas parseadas exitosamente: {self.successful_parses}, "
                        f"Fallidas: {self.failed_parses}")
        return self.unit_costs
    
    def _process_financial_table(self, table: pd.DataFrame, 
                                nodes: Dict[str, MetaNode]) -> None:
        """Process a single financial table"""
        # Try to identify relevant columns
        amount_pattern = re.compile(
            self.config.get('patterns.financial_headers', r'PRESUPUESTO|VALOR|MONTO'),
            re.IGNORECASE
        )
        program_pattern = re.compile(r'PROGRAMA|META|CÓDIGO', re.IGNORECASE)
        
        amount_col = None
        program_col = None
        
        # Search in column names
        for col in table.columns:
            col_str = str(col)
            if amount_pattern.search(col_str) and not amount_col:
                amount_col = col
            if program_pattern.search(col_str) and not program_col:
                program_col = col
        
        # If not found in column names, search in first row
        if not amount_col or not program_col:
            first_row = table.iloc[0]
            for i, val in enumerate(first_row):
                val_str = str(val)
                if amount_pattern.search(val_str) and not amount_col:
                    amount_col = i
                    table.columns = table.iloc[0]
                    table = table[1:]
                if program_pattern.search(val_str) and not program_col:
                    program_col = i
                    table.columns = table.iloc[0]
                    table = table[1:]
        
        if amount_col is None or program_col is None:
            self.logger.warning("No se encontraron columnas financieras relevantes")
            return
        
        for _, row in table.iterrows():
            try:
                program_id = str(row[program_col]).strip().upper()
                amount = self._parse_amount(row[amount_col])
                
                if amount and program_id:
                    matched_node = self._match_program_to_node(program_id, nodes)
                    if matched_node:
                        self.financial_data[matched_node] = {
                            'allocation': amount,
                            'source': 'budget_table'
                        }
                        
                        # Update node
                        nodes[matched_node].financial_allocation = amount
                        
                        # Calculate unit cost if possible
                        node = nodes.get(matched_node)
                        if node and node.target:
                            try:
                                target_val = float(str(node.target).replace(',', '').replace('%', ''))
                                if target_val > 0:
                                    unit_cost = amount / target_val
                                    self.unit_costs[matched_node] = unit_cost
                                    nodes[matched_node].unit_cost = unit_cost
                            except (ValueError, TypeError):
                                pass
                                
            except Exception as e:
                self.logger.debug(f"Error procesando fila financiera: {e}")
                continue
    
    def _parse_amount(self, value: Any) -> Optional[float]:
        """Parse monetary amount from various formats"""
        if pd.isna(value):
            return None
        
        try:
            clean_value = str(value).replace('$', '').replace(',', '').replace(' ', '').replace('.', '')
            # Handle millions/thousands notation
            if 'M' in clean_value.upper() or 'MILLONES' in clean_value.upper():
                clean_value = clean_value.upper().replace('M', '').replace('ILLONES', '')
                return float(clean_value) * 1_000_000
            return float(clean_value)
        except (ValueError, TypeError):
            return None
    
    def _match_program_to_node(self, program_id: str, 
                               nodes: Dict[str, MetaNode]) -> Optional[str]:
        """Match program ID to existing node using fuzzy matching
        
        Enhanced for D1-Q3 / D3-Q3 Financial Traceability:
        - Implements confidence penalty if fuzzy match ratio < 100
        - Reduces node.financial_allocation confidence by 15% for imperfect matches
        - Tracks match quality for overall financial traceability scoring
        """
        if program_id in nodes:
            # Perfect match - no penalty
            return program_id
        
        # Try fuzzy matching
        best_match = process.extractOne(
            program_id,
            nodes.keys(),
            scorer=fuzz.ratio,
            score_cutoff=80
        )
        
        if best_match:
            matched_node_id = best_match[0]
            match_ratio = best_match[1]
            
            # D1-Q3 / D3-Q3: Apply confidence penalty for non-perfect matches
            if match_ratio < 100:
                penalty_factor = 0.85  # 15% reduction as specified
                node = nodes[matched_node_id]
                
                # Track original allocation before penalty
                if not hasattr(node, '_original_financial_allocation'):
                    node._original_financial_allocation = node.financial_allocation
                
                # Apply penalty to financial allocation confidence
                if node.financial_allocation:
                    penalized_allocation = node.financial_allocation * penalty_factor
                    self.logger.debug(
                        f"Fuzzy match penalty applied to {matched_node_id}: "
                        f"ratio={match_ratio}, penalty={penalty_factor:.2f}, "
                        f"allocation {node.financial_allocation:.0f} -> {penalized_allocation:.0f}"
                    )
                    node.financial_allocation = penalized_allocation
                
                # Store match confidence for D1-Q3 / D3-Q3 scoring
                if not hasattr(node, 'financial_match_confidence'):
                    node.financial_match_confidence = match_ratio / 100.0
                else:
                    # Average if multiple matches
                    node.financial_match_confidence = (node.financial_match_confidence + match_ratio / 100.0) / 2
            
            return matched_node_id
        
        return None
    
    def _perform_counterfactual_budget_check(self, nodes: Dict[str, MetaNode], 
                                            graph: nx.DiGraph) -> None:
        """
        Harmonic Front 3 - Enhancement 5: Counterfactual Sufficiency Test for D3-Q3
        
        Tests minimal sufficiency: if resource X (BPIN code) were removed, would the
        mechanism (Product) still execute? Only boosts budget traceability score if
        allocation is tied to a specific project.
        
        For D3-Q3 (Traceability/Resources): ensures funding is necessary for the mechanism
        and prevents false positives from generic or disconnected budget entries.
        """
        d3_q3_scores = {}
        
        for node_id, node in nodes.items():
            if node.type != 'producto':
                continue
            
            # Check if node has financial allocation
            has_budget = node.financial_allocation is not None and node.financial_allocation > 0
            
            # Check if node has entity-activity (mechanism)
            has_mechanism = node.entity_activity is not None
            
            # Check if node has dependencies (successors in graph)
            successors = list(graph.successors(node_id)) if graph.has_node(node_id) else []
            has_dependencies = len(successors) > 0
            
            # Counterfactual test: Would mechanism still execute without this budget?
            # Check if there are alternative funding sources or generic allocations
            financial_source = self.financial_data.get(node_id, {}).get('source', 'unknown')
            is_specific_allocation = financial_source == 'budget_table'  # From specific table entry
            
            # Calculate counterfactual necessity score
            # High score = budget is necessary for execution
            # Low score = budget may be generic/disconnected
            necessity_score = 0.0
            
            if has_budget and has_mechanism:
                necessity_score += 0.40  # Budget + mechanism present
            
            if has_budget and has_dependencies:
                necessity_score += 0.30  # Budget supports downstream goals
            
            if is_specific_allocation:
                necessity_score += 0.30  # Specific allocation (not generic)
            
            # D3-Q3 quality criteria
            d3_q3_quality = 'insuficiente'
            if necessity_score >= 0.85:
                d3_q3_quality = 'excelente'
            elif necessity_score >= 0.70:
                d3_q3_quality = 'bueno'
            elif necessity_score >= 0.50:
                d3_q3_quality = 'aceptable'
            
            d3_q3_scores[node_id] = {
                'necessity_score': necessity_score,
                'd3_q3_quality': d3_q3_quality,
                'has_budget': has_budget,
                'has_mechanism': has_mechanism,
                'has_dependencies': has_dependencies,
                'is_specific_allocation': is_specific_allocation,
                'counterfactual_sufficient': necessity_score < 0.50,  # Would still execute without budget
                'budget_necessary': necessity_score >= 0.70  # Budget is necessary
            }
            
            # Store in node for later retrieval
            node.audit_flags = node.audit_flags or []
            if necessity_score < 0.50:
                node.audit_flags.append('budget_not_necessary')
                self.logger.warning(f"D3-Q3: {node_id} may execute without allocated budget (score={necessity_score:.2f})")
            elif necessity_score >= 0.85:
                node.audit_flags.append('budget_well_traced')
                self.logger.info(f"D3-Q3: {node_id} has well-traced, necessary budget (score={necessity_score:.2f})")
        
        # Store aggregate D3-Q3 metrics
        self.d3_q3_analysis = {
            'node_scores': d3_q3_scores,
            'total_products_analyzed': len(d3_q3_scores),
            'well_traced_count': sum(1 for s in d3_q3_scores.values() if s['d3_q3_quality'] == 'excelente'),
            'average_necessity_score': sum(s['necessity_score'] for s in d3_q3_scores.values()) / max(len(d3_q3_scores), 1)
        }
        
        self.logger.info(f"D3-Q3 Counterfactual Budget Check completed: "
                        f"{self.d3_q3_analysis['well_traced_count']}/{len(d3_q3_scores)} "
                        f"products with excellent traceability")
        if best_match:
            return best_match[0]
        
        return None


class OperationalizationAuditor:
    """Audit operationalization quality"""
    
    def __init__(self, config: ConfigLoader) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.verb_sequences = config.get('verb_sequences', {})
        self.audit_results: Dict[str, AuditResult] = {}
        self.sequence_warnings: List[str] = []
    
    def audit_evidence_traceability(self, nodes: Dict[str, MetaNode]) -> Dict[str, AuditResult]:
        """Audit evidence traceability for all nodes
        
        Enhanced with D3-Q1 Ficha Técnica validation:
        - Cross-checks baseline/target against extracted quantitative_claims
        - Verifies DNP INDICATOR_STRUCTURE compliance for producto nodes
        - Scores 'Excelente' only if ≥80% of productos pass full audit
        """
        # Import for quantitative claims extraction
        try:
            from contradiction_deteccion import PolicyContradictionDetectorV2
            has_detector = True
        except ImportError:
            has_detector = False
            self.logger.warning("PolicyContradictionDetectorV2 not available for quantitative claims validation")
        
        producto_nodes_count = 0
        producto_nodes_passed = 0
        
        for node_id, node in nodes.items():
            result: AuditResult = {
                'passed': True,
                'warnings': [],
                'errors': [],
                'recommendations': []
            }
            
            # Track producto nodes for D3-Q1 scoring
            if node.type == 'producto':
                producto_nodes_count += 1
            
            # Extract quantitative claims from node text if detector available
            quantitative_claims = []
            if has_detector:
                try:
                    # Create temporary detector instance
                    detector = PolicyContradictionDetectorV2(device='cpu')
                    quantitative_claims = detector._extract_structured_quantitative_claims(node.text)
                except Exception as e:
                    self.logger.debug(f"Could not extract quantitative claims: {e}")
            
            # Check baseline
            baseline_valid = False
            if not node.baseline or str(node.baseline).upper() in ['ND', 'POR DEFINIR', 'N/A', 'NONE']:
                result['errors'].append(f"Línea base no definida para {node_id}")
                result['passed'] = False
                node.rigor_status = 'débil'
                node.audit_flags.append('sin_linea_base')
            else:
                baseline_valid = True
                # Cross-check baseline against quantitative claims (D3-Q1)
                if quantitative_claims:
                    baseline_in_claims = any(
                        claim.get('type') in ['indicator', 'target', 'percentage', 'beneficiaries'] 
                        for claim in quantitative_claims
                    )
                    if not baseline_in_claims:
                        result['warnings'].append(f"Línea base no verificada en claims cuantitativos para {node_id}")
            
            # Check target
            target_valid = False
            if not node.target or str(node.target).upper() in ['ND', 'POR DEFINIR', 'N/A', 'NONE']:
                result['errors'].append(f"Meta no definida para {node_id}")
                result['passed'] = False
                node.rigor_status = 'débil'
                node.audit_flags.append('sin_meta')
            else:
                target_valid = True
                # Cross-check target against quantitative claims (D3-Q1)
                if quantitative_claims:
                    meta_in_claims = any(
                        claim.get('type') == 'target' or 'meta' in claim.get('context', '').lower()
                        for claim in quantitative_claims
                    )
                    if not meta_in_claims:
                        result['warnings'].append(f"Meta no verificada en claims cuantitativos para {node_id}")
            
            # D3-Q1 Ficha Técnica compliance check for producto nodes
            if node.type == 'producto':
                # Check if has all minimum DNP INDICATOR_STRUCTURE elements
                has_complete_ficha = (
                    baseline_valid and 
                    target_valid and 
                    'sin_linea_base' not in node.audit_flags and
                    'sin_meta' not in node.audit_flags
                )
                
                if has_complete_ficha and quantitative_claims:
                    # Node passes D3-Q1 compliance
                    producto_nodes_passed += 1
                    result['recommendations'].append(f"D3-Q1 Ficha Técnica completa para {node_id}")
                elif has_complete_ficha:
                    # Has baseline/target but no quantitative claims verification
                    producto_nodes_passed += 0.5  # Partial credit
                    result['warnings'].append(f"D3-Q1 parcial: Ficha básica sin verificación cuantitativa en {node_id}")
            
            # Check responsible entity
            if not node.responsible_entity:
                result['warnings'].append(f"Entidad responsable no identificada para {node_id}")
                node.audit_flags.append('sin_responsable')
            
            # Check financial traceability
            if not node.financial_allocation:
                result['warnings'].append(f"Sin trazabilidad financiera para {node_id}")
                node.audit_flags.append('sin_presupuesto')
            
            # Set rigor status if passed all checks
            if result['passed'] and len(result['warnings']) == 0:
                node.rigor_status = 'fuerte'
            
            self.audit_results[node_id] = result
        
        # Calculate D3-Q1 compliance score
        if producto_nodes_count > 0:
            d3_q1_compliance_pct = (producto_nodes_passed / producto_nodes_count) * 100
            self.logger.info(f"D3-Q1 Ficha Técnica Compliance: {d3_q1_compliance_pct:.1f}% "
                           f"({producto_nodes_passed}/{producto_nodes_count} productos)")
            
            if d3_q1_compliance_pct >= 80:
                self.logger.info("D3-Q1 Score: EXCELENTE (≥80% productos con Ficha Técnica completa)")
            elif d3_q1_compliance_pct >= 60:
                self.logger.info("D3-Q1 Score: BUENO (60-80% compliance)")
            else:
                self.logger.warning("D3-Q1 Score: INSUFICIENTE (<60% compliance)")
        
        passed_count = sum(1 for r in self.audit_results.values() if r['passed'])
        self.logger.info(f"Auditoría de trazabilidad: {passed_count}/{len(nodes)} nodos aprobados")
        
        return self.audit_results
    
    def audit_sequence_logic(self, graph: nx.DiGraph) -> List[str]:
        """Audit logical sequence of activities"""
        warnings = []
        
        # Group nodes by program
        programs: Dict[str, List[str]] = defaultdict(list)
        for node_id in graph.nodes():
            node_data = graph.nodes[node_id]
            if node_data.get('type') == 'programa':
                for successor in graph.successors(node_id):
                    if graph.nodes[successor].get('type') == 'producto':
                        programs[node_id].append(successor)
        
        # Check sequence within each program
        for program_id, product_goals in programs.items():
            if len(product_goals) < 2:
                continue
            
            activities = []
            for goal_id in product_goals:
                node = graph.nodes[goal_id]
                ea = node.get('entity_activity')
                if ea and isinstance(ea, dict):
                    verb = ea.get('verb_lemma', '')
                    sequence_num = self.verb_sequences.get(verb, 999)
                    activities.append((goal_id, verb, sequence_num))
            
            # Check for sequence violations
            activities.sort(key=lambda x: x[2])
            for i in range(len(activities) - 1):
                if activities[i][2] > activities[i + 1][2]:
                    warning = (f"Violación de secuencia en {program_id}: "
                             f"{activities[i][1]} ({activities[i][0]}) "
                             f"antes de {activities[i + 1][1]} ({activities[i + 1][0]})")
                    warnings.append(warning)
                    self.logger.warning(warning)
        
        self.sequence_warnings = warnings
        return warnings
    
    def bayesian_counterfactual_audit(self, nodes: Dict[str, MetaNode], 
                                     graph: nx.DiGraph,
                                     historical_data: Optional[Dict[str, Any]] = None,
                                     pdet_alignment: Optional[float] = None) -> Dict[str, Any]:
        """
        AGUJA III: El Auditor Contrafactual Bayesiano
        Perform counterfactual audit using Bayesian causal reasoning
        
        Harmonic Front 3: Enhanced to consume pdet_alignment scores for D4-Q5 and D5-Q4 integration
        """
        self.logger.info("Iniciando auditoría contrafactual Bayesiana...")
        
        # Build implicit Structural Causal Model (SCM)
        scm_dag = self._build_normative_dag()
        
        # Initialize historical priors
        if historical_data is None:
            historical_data = self._get_default_historical_priors()
        
        # Audit results by layers
        layer1_results = self._audit_direct_evidence(nodes, scm_dag, historical_data)
        layer2_results = self._audit_causal_implications(nodes, graph, layer1_results)
        layer3_results = self._audit_systemic_risk(nodes, graph, layer1_results, layer2_results, pdet_alignment)
        
        # Generate optimal remediation recommendations
        recommendations = self._generate_optimal_remediations(
            layer1_results, layer2_results, layer3_results
        )
        
        audit_report = {
            'direct_evidence': layer1_results,
            'causal_implications': layer2_results,
            'systemic_risk': layer3_results,
            'recommendations': recommendations,
            'summary': {
                'total_nodes': len(nodes),
                'critical_omissions': sum(1 for r in layer1_results.values() 
                                        if r.get('omission_severity') == 'critical'),
                'expected_success_probability': layer3_results.get('success_probability', 0.0),
                'risk_score': layer3_results.get('risk_score', 0.0)
            }
        }
        
        self.logger.info(f"Auditoría contrafactual completada: "
                        f"{audit_report['summary']['critical_omissions']} omisiones críticas detectadas")
        
        return audit_report
    
    def _build_normative_dag(self) -> nx.DiGraph:
        """Build normative DAG of expected relationships in well-formed plans"""
        dag = nx.DiGraph()
        
        # Define normative structure
        # Each goal type should have these attributes
        dag.add_node('baseline', type='required_attribute')
        dag.add_node('target', type='required_attribute')
        dag.add_node('entity', type='required_attribute')
        dag.add_node('budget', type='recommended_attribute')
        dag.add_node('mechanism', type='recommended_attribute')
        dag.add_node('timeline', type='optional_attribute')
        dag.add_node('risk_factors', type='optional_attribute')
        
        # Causal relationships
        dag.add_edge('baseline', 'target', relation='defines_gap')
        dag.add_edge('entity', 'mechanism', relation='executes')
        dag.add_edge('budget', 'mechanism', relation='enables')
        dag.add_edge('mechanism', 'target', relation='achieves')
        dag.add_edge('risk_factors', 'target', relation='threatens')
        
        return dag
    
    def _get_default_historical_priors(self) -> Dict[str, Any]:
        """Get default historical priors if no data is available"""
        return {
            'entity_presence_success_rate': 0.94,
            'baseline_presence_success_rate': 0.89,
            'target_presence_success_rate': 0.92,
            'budget_presence_success_rate': 0.78,
            'mechanism_presence_success_rate': 0.65,
            'complete_documentation_success_rate': 0.82,
            'node_type_success_rates': {
                'producto': 0.85,
                'resultado': 0.72,
                'impacto': 0.58
            }
        }
    
    def _audit_direct_evidence(self, nodes: Dict[str, MetaNode],
                               scm_dag: nx.DiGraph,
                               historical_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        """Layer 1: Audit direct evidence of required components
        
        Enhanced with highly specific Bayesian priors for rare evidence items.
        Example: D2-Q4 risk matrix, D5-Q5 unwanted effects are rare in poor PDMs.
        """
        results = {}
        
        # Load highly specific priors for rare evidence types
        # D2-Q4: Risk matrices are rare in poor PDMs (high probative value as Smoking Gun)
        rare_evidence_priors = {
            'risk_matrix': {
                'prior_alpha': 1.5,  # Low alpha = rare occurrence
                'prior_beta': 12.0,  # High beta = high failure rate when absent
                'keywords': ['matriz de riesgo', 'análisis de riesgo', 'gestión de riesgo', 'riesgos identificados']
            },
            'unwanted_effects': {
                'prior_alpha': 1.8,  # D5-Q5: Effects analysis is also rare
                'prior_beta': 10.5,
                'keywords': ['efectos no deseados', 'efectos adversos', 'impactos negativos', 'consecuencias no previstas']
            },
            'theory_of_change': {
                'prior_alpha': 1.2,
                'prior_beta': 15.0,
                'keywords': ['teoría de cambio', 'teoría del cambio', 'cadena causal', 'modelo lógico']
            }
        }
        
        for node_id, node in nodes.items():
            omissions = []
            omission_probs = {}
            rare_evidence_found = {}
            
            # Check for rare, high-value evidence in node text
            node_text_lower = node.text.lower()
            for evidence_type, prior_config in rare_evidence_priors.items():
                if any(kw in node_text_lower for kw in prior_config['keywords']):
                    # Rare evidence found! Strong Smoking Gun
                    rare_evidence_found[evidence_type] = {
                        'prior_alpha': prior_config['prior_alpha'],
                        'prior_beta': prior_config['prior_beta'],
                        'posterior_strength': prior_config['prior_alpha'] / (prior_config['prior_alpha'] + prior_config['prior_beta'])
                    }
                    self.logger.info(f"Rare evidence '{evidence_type}' found in {node_id} - Strong Smoking Gun!")
            
            # Check baseline
            if not node.baseline or str(node.baseline).upper() in ['ND', 'POR DEFINIR', 'N/A', 'NONE']:
                p_failure_given_omission = 1.0 - historical_data.get('baseline_presence_success_rate', 0.89)
                omissions.append('baseline')
                omission_probs['baseline'] = p_failure_given_omission
            
            # Check target
            if not node.target or str(node.target).upper() in ['ND', 'POR DEFINIR', 'N/A', 'NONE']:
                p_failure_given_omission = 1.0 - historical_data.get('target_presence_success_rate', 0.92)
                omissions.append('target')
                omission_probs['target'] = p_failure_given_omission
            
            # Check entity
            if not node.responsible_entity:
                p_failure_given_omission = 1.0 - historical_data.get('entity_presence_success_rate', 0.94)
                omissions.append('entity')
                omission_probs['entity'] = p_failure_given_omission
            
            # Check budget
            if not node.financial_allocation:
                p_failure_given_omission = 1.0 - historical_data.get('budget_presence_success_rate', 0.78)
                omissions.append('budget')
                omission_probs['budget'] = p_failure_given_omission
            
            # Check mechanism
            if not node.entity_activity:
                p_failure_given_omission = 1.0 - historical_data.get('mechanism_presence_success_rate', 0.65)
                omissions.append('mechanism')
                omission_probs['mechanism'] = p_failure_given_omission
            
            # Determine severity
            severity = 'none'
            if omission_probs:
                max_failure_prob = max(omission_probs.values())
                if max_failure_prob > 0.15:
                    severity = 'critical'
                elif max_failure_prob > 0.10:
                    severity = 'high'
                elif max_failure_prob > 0.05:
                    severity = 'medium'
                else:
                    severity = 'low'
            
            results[node_id] = {
                'omissions': omissions,
                'omission_probabilities': omission_probs,
                'omission_severity': severity,
                'node_type': node.type,
                'rare_evidence_found': rare_evidence_found  # Add rare evidence to results
            }
        
        return results
    
    def _audit_causal_implications(self, nodes: Dict[str, MetaNode],
                                   graph: nx.DiGraph,
                                   direct_evidence: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """Layer 2: Audit causal implications of omissions"""
        implications = {}
        
        for node_id, node in nodes.items():
            node_omissions = direct_evidence[node_id]['omissions']
            causal_effects = {}
            
            # If baseline is missing
            if 'baseline' in node_omissions:
                # P(target_miscalibrated | missing_baseline)
                causal_effects['target_miscalibration'] = {
                    'probability': 0.73,
                    'description': 'Sin línea base, la meta probablemente está mal calibrada'
                }
            
            # If entity and high budget are missing
            if 'entity' in node_omissions and node.financial_allocation and node.financial_allocation > 1000000:
                causal_effects['implementation_failure'] = {
                    'probability': 0.89,
                    'description': 'Alto presupuesto sin entidad responsable indica alto riesgo de falla'
                }
            elif 'entity' in node_omissions:
                causal_effects['implementation_failure'] = {
                    'probability': 0.65,
                    'description': 'Sin entidad responsable, la implementación es incierta'
                }
            
            # If mechanism is missing
            if 'mechanism' in node_omissions:
                causal_effects['unclear_pathway'] = {
                    'probability': 0.70,
                    'description': 'Sin mecanismo definido, la vía causal es opaca'
                }
            
            # Check downstream effects
            successors = list(graph.successors(node_id)) if graph.has_node(node_id) else []
            if node_omissions and successors:
                causal_effects['cascade_risk'] = {
                    'probability': min(0.95, 0.4 + 0.1 * len(node_omissions)),
                    'affected_nodes': successors,
                    'description': f'Omisiones pueden afectar {len(successors)} nodos dependientes'
                }
            
            implications[node_id] = {
                'causal_effects': causal_effects,
                'total_risk': sum(e['probability'] for e in causal_effects.values()) / max(len(causal_effects), 1)
            }
        
        return implications
    
    def _audit_systemic_risk(self, nodes: Dict[str, MetaNode],
                            graph: nx.DiGraph,
                            direct_evidence: Dict[str, Dict[str, Any]],
                            causal_implications: Dict[str, Dict[str, Any]],
                            pdet_alignment: Optional[float] = None) -> Dict[str, Any]:
        """
        AUDIT POINT 2.3: Policy Alignment Dual Constraint
        Layer 3: Calculate systemic risk from accumulated omissions
        
        Harmonic Front 3 - Enhancement 1: Alignment and Systemic Risk Linkage
        Incorporates Policy Alignment scores (PND, ODS, RRI) as variable in systemic risk.
        
        For D5-Q4 (Riesgos Sistémicos) and D4-Q5 (Alineación):
        - If pdet_alignment ≤ 0.60, applies 1.2× multiplier to risk_score
        - Excelente on D5-Q4 requires risk_score < 0.10
        
        Implements dual constraints integrating macro-micro causality per Lieberman 2015.
        """
        
        # Identify critical nodes (high centrality)
        if graph.number_of_nodes() > 0:
            try:
                centrality = nx.betweenness_centrality(graph)
            except:
                centrality = {n: 0.5 for n in graph.nodes()}
        else:
            centrality = {}
        
        # Calculate P(cascade_failure | omission_set)
        critical_omissions = []
        for node_id, evidence in direct_evidence.items():
            if evidence['omission_severity'] in ['critical', 'high']:
                node_centrality = centrality.get(node_id, 0.5)
                critical_omissions.append({
                    'node_id': node_id,
                    'severity': evidence['omission_severity'],
                    'centrality': node_centrality,
                    'omissions': evidence['omissions']
                })
        
        # Calculate systemic risk
        if critical_omissions:
            # Weighted by centrality
            risk_score = sum(
                (1.0 if om['severity'] == 'critical' else 0.7) * (om['centrality'] + 0.1)
                for om in critical_omissions
            ) / len(nodes)
        else:
            risk_score = 0.0
        
        # AUDIT POINT 2.3: Policy Alignment Dual Constraint
        # If pdet_alignment ≤ 0.60, apply 1.2× multiplier to risk_score
        # This enforces integration between D4-Q5 (Alineación) and D5-Q4 (Riesgos Sistémicos)
        alignment_penalty_applied = False
        alignment_threshold = 0.60
        alignment_multiplier = 1.2
        
        if pdet_alignment is not None and pdet_alignment <= alignment_threshold:
            original_risk = risk_score
            risk_score = risk_score * alignment_multiplier
            alignment_penalty_applied = True
            self.logger.warning(
                f"ALIGNMENT PENALTY (D5-Q4): pdet_alignment={pdet_alignment:.2f} ≤ {alignment_threshold}, "
                f"risk_score escalated from {original_risk:.3f} to {risk_score:.3f} "
                f"(multiplier: {alignment_multiplier}×). Dual constraint per Lieberman 2015."
            )
        
        # Calculate P(success | current_state)
        total_omissions = sum(len(e['omissions']) for e in direct_evidence.values())
        total_possible = len(nodes) * 5  # 5 key attributes per node
        completeness = 1.0 - (total_omissions / max(total_possible, 1))
        
        # Success probability (simplified Bayesian)
        base_success_rate = 0.70
        success_probability = base_success_rate * completeness
        
        # D5-Q4 quality criteria check (AUDIT POINT 2.3)
        # Excellent requires risk_score < 0.10 (matching ODS benchmarks per UN 2020)
        d5_q4_quality = 'insuficiente'
        risk_threshold_excellent = 0.10
        risk_threshold_good = 0.20
        risk_threshold_acceptable = 0.35
        
        if risk_score < risk_threshold_excellent:
            d5_q4_quality = 'excelente'
        elif risk_score < risk_threshold_good:
            d5_q4_quality = 'bueno'
        elif risk_score < risk_threshold_acceptable:
            d5_q4_quality = 'aceptable'
        
        # Flag if alignment is causing quality failure
        alignment_causing_failure = (
            alignment_penalty_applied and 
            original_risk < risk_threshold_excellent and 
            risk_score >= risk_threshold_excellent
        )
        
        return {
            'risk_score': min(1.0, risk_score),
            'success_probability': success_probability,
            'critical_omissions': critical_omissions,
            'completeness': completeness,
            'total_omissions': total_omissions,
            'pdet_alignment': pdet_alignment,
            'alignment_penalty_applied': alignment_penalty_applied,
            'alignment_threshold': alignment_threshold,
            'alignment_multiplier': alignment_multiplier,
            'alignment_causing_failure': alignment_causing_failure,
            'd5_q4_quality': d5_q4_quality,
            'd4_q5_alignment_score': pdet_alignment,
            'risk_thresholds': {
                'excellent': risk_threshold_excellent,
                'good': risk_threshold_good,
                'acceptable': risk_threshold_acceptable
            }
        }
    
    def _generate_optimal_remediations(self, 
                                      direct_evidence: Dict[str, Dict[str, Any]],
                                      causal_implications: Dict[str, Dict[str, Any]],
                                      systemic_risk: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate prioritized remediation recommendations"""
        remediations = []
        
        # Calculate expected value of information for each remediation
        for node_id, evidence in direct_evidence.items():
            if not evidence['omissions']:
                continue
            
            for omission in evidence['omissions']:
                # Estimate impact
                omission_prob = evidence['omission_probabilities'].get(omission, 0.1)
                causal_risk = causal_implications[node_id]['total_risk']
                
                # Expected value = P(failure_avoided) * Impact
                expected_value = omission_prob * (1 + causal_risk)
                
                # Effort estimate (simplified)
                effort_map = {
                    'baseline': 3,  # Moderate effort to research
                    'target': 2,    # Low effort to define
                    'entity': 2,    # Low effort to assign
                    'budget': 4,    # Higher effort to allocate
                    'mechanism': 5  # Highest effort to design
                }
                effort = effort_map.get(omission, 3)
                
                # Priority = Expected Value / Effort
                priority = expected_value / effort
                
                remediations.append({
                    'node_id': node_id,
                    'omission': omission,
                    'severity': evidence['omission_severity'],
                    'expected_value': expected_value,
                    'effort': effort,
                    'priority': priority,
                    'recommendation': self._get_remediation_text(omission, node_id)
                })
        
        # Sort by priority (descending)
        remediations.sort(key=lambda x: x['priority'], reverse=True)
        
        return remediations
    
    def _get_remediation_text(self, omission: str, node_id: str) -> str:
        """Get specific remediation text for an omission"""
        texts = {
            'baseline': f"Definir línea base cuantitativa para {node_id} basada en diagnóstico actual",
            'target': f"Especificar meta cuantitativa alcanzable para {node_id} con horizonte temporal",
            'entity': f"Asignar entidad responsable clara para la ejecución de {node_id}",
            'budget': f"Asignar recursos presupuestarios específicos a {node_id}",
            'mechanism': f"Documentar mecanismo causal (Entidad-Actividad) para {node_id}"
        }
        return texts.get(omission, f"Completar {omission} para {node_id}")



class BayesianMechanismInference:
    """
    AGUJA II: El Modelo Generativo de Mecanismos
    Hierarchical Bayesian model for causal mechanism inference
    
    F1.2 ARCHITECTURAL REFACTORING:
    This class now integrates with refactored Bayesian engine components:
    - BayesianPriorBuilder: Construye priors adaptativos (AGUJA I)
    - BayesianSamplingEngine: Ejecuta MCMC sampling (AGUJA II)
    - NecessitySufficiencyTester: Ejecuta Hoop Tests (AGUJA III)
    
    The refactored components provide:
    - Crystal-clear separation of concerns
    - Trivial unit testing
    - Explicit compliance with Fronts B and C
    
    Legacy methods are preserved for backward compatibility.
    """
    
    def __init__(self, config: ConfigLoader, nlp_model: spacy.Language) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.nlp = nlp_model
        
        # F1.2: Initialize refactored Bayesian engine adapter if available
        if REFACTORED_BAYESIAN_AVAILABLE:
            try:
                self.bayesian_adapter = BayesianEngineAdapter(config, nlp_model)
                if self.bayesian_adapter.is_available():
                    self.logger.info("✓ Usando motor Bayesiano refactorizado (F1.2)")
                    self._log_refactored_components()
                else:
                    self.bayesian_adapter = None
            except Exception as e:
                self.logger.warning(f"Error inicializando motor refactorizado: {e}")
                self.bayesian_adapter = None
        else:
            self.bayesian_adapter = None
        
        # Load mechanism type hyperpriors from configuration (externalized)
        self.mechanism_type_priors = {
            'administrativo': self.config.get_mechanism_prior('administrativo'),
            'tecnico': self.config.get_mechanism_prior('tecnico'),
            'financiero': self.config.get_mechanism_prior('financiero'),
            'politico': self.config.get_mechanism_prior('politico'),
            'mixto': self.config.get_mechanism_prior('mixto')
        }
        
        # Typical activity sequences by mechanism type
        # These could also be externalized if needed for domain-specific customization
        self.mechanism_sequences = {
            'administrativo': ['planificar', 'coordinar', 'gestionar', 'supervisar'],
            'tecnico': ['diagnosticar', 'diseñar', 'implementar', 'evaluar'],
            'financiero': ['asignar', 'ejecutar', 'auditar', 'reportar'],
            'politico': ['concertar', 'negociar', 'aprobar', 'promulgar']
        }
        
        # Track inferred mechanisms
        self.inferred_mechanisms: Dict[str, Dict[str, Any]] = {}
    
    def _log_refactored_components(self) -> None:
        """Log status of refactored Bayesian components (F1.2)"""
        if self.bayesian_adapter:
            status = self.bayesian_adapter.get_component_status()
            self.logger.info("  - BayesianPriorBuilder: " + 
                           ("✓" if status['prior_builder_ready'] else "✗"))
            self.logger.info("  - BayesianSamplingEngine: " + 
                           ("✓" if status['sampling_engine_ready'] else "✗"))
            self.logger.info("  - NecessitySufficiencyTester: " + 
                           ("✓" if status['necessity_tester_ready'] else "✗"))
    
    def infer_mechanisms(self, nodes: Dict[str, MetaNode], 
                        text: str) -> Dict[str, Dict[str, Any]]:
        """
        Infer latent causal mechanisms using hierarchical Bayesian modeling
        
        HARMONIC FRONT 4 ENHANCEMENT:
        - Tracks mean mechanism_type uncertainty for quality criteria
        - Reports uncertainty reduction metrics
        """
        self.logger.info("Iniciando inferencia Bayesiana de mecanismos...")
        
        # Focus on 'producto' nodes which should have mechanisms
        product_nodes = {nid: n for nid, n in nodes.items() if n.type == 'producto'}
        
        # Track uncertainties for mean calculation
        mechanism_uncertainties = []
        
        for node_id, node in product_nodes.items():
            mechanism = self._infer_single_mechanism(node, text, nodes)
            self.inferred_mechanisms[node_id] = mechanism
            
            # Track mechanism type uncertainty for quality criteria
            if 'uncertainty' in mechanism:
                mech_type_uncertainty = mechanism['uncertainty'].get('mechanism_type', 1.0)
                mechanism_uncertainties.append(mech_type_uncertainty)
        
        # Calculate mean mechanism uncertainty for Harmonic Front 4 quality criteria
        mean_mech_uncertainty = (
            np.mean(mechanism_uncertainties) if mechanism_uncertainties else 1.0
        )
        
        self.logger.info(f"Mecanismos inferidos: {len(self.inferred_mechanisms)}")
        self.logger.info(f"Mean mechanism_type uncertainty: {mean_mech_uncertainty:.4f}")
        
        # Store for reporting
        self._mean_mechanism_uncertainty = mean_mech_uncertainty
        
        return self.inferred_mechanisms
    
    
    def _infer_single_mechanism(self, node: MetaNode, text: str, 
                                all_nodes: Dict[str, MetaNode]) -> Dict[str, Any]:
        """Infer mechanism for a single product node"""
        # Extract observations from text
        observations = self._extract_observations(node, text)
        
        # Level 3: Sample mechanism type from hyperprior
        mechanism_type_posterior = self._infer_mechanism_type(observations)
        
        # Level 2: Infer activity sequence given mechanism type
        sequence_posterior = self._infer_activity_sequence(
            observations, mechanism_type_posterior
        )
        
        # Level 1: Calculate coherence factor
        coherence_score = self._calculate_coherence_factor(
            node, observations, all_nodes
        )
        
        # Validation tests
        sufficiency = self._test_sufficiency(node, observations)
        necessity = self._test_necessity(node, observations)
        
        # Quantify uncertainty
        uncertainty = self._quantify_uncertainty(
            mechanism_type_posterior, sequence_posterior, coherence_score
        )
        
        # Detect gaps
        gaps = self._detect_gaps(node, observations, uncertainty)
        
        return {
            'mechanism_type': mechanism_type_posterior,
            'activity_sequence': sequence_posterior,
            'coherence_score': coherence_score,
            'sufficiency_test': sufficiency,
            'necessity_test': necessity,
            'uncertainty': uncertainty,
            'gaps': gaps,
            'observations': observations
        }
    
    def _extract_observations(self, node: MetaNode, text: str) -> Dict[str, Any]:
        """Extract textual observations related to the mechanism"""
        # Find node context in text
        node_pattern = re.escape(node.id)
        matches = list(re.finditer(node_pattern, text, re.IGNORECASE))
        
        observations = {
            'entity_activity': None,
            'verbs': [],
            'entities': [],
            'budget': node.financial_allocation,
            'context_snippets': []
        }
        
        if node.entity_activity:
            observations['entity_activity'] = {
                'entity': node.entity_activity.entity,
                'activity': node.entity_activity.activity,
                'verb_lemma': node.entity_activity.verb_lemma
            }
        
        # Extract context around node mentions
        for match in matches[:3]:  # Limit to first 3 occurrences
            start = max(0, match.start() - 300)
            end = min(len(text), match.end() + 300)
            context = text[start:end]
            
            # Process with spaCy
            doc = self.nlp(context)
            
            # Extract verbs
            verbs = [token.lemma_ for token in doc if token.pos_ == 'VERB']
            observations['verbs'].extend(verbs)
            
            # Extract entities
            entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PER']]
            observations['entities'].extend(entities)
            
            observations['context_snippets'].append(context[:200])
        
        return observations
    
    def _infer_mechanism_type(self, observations: Dict[str, Any]) -> Dict[str, float]:
        """Infer mechanism type using Bayesian updating"""
        # Start with hyperprior
        posterior = dict(self.mechanism_type_priors)
        
        # Get Laplace smoothing parameter from configuration
        laplace_smooth = self.config.get_bayesian_threshold('laplace_smoothing')
        
        # Update based on observed verbs
        observed_verbs = set(observations.get('verbs', []))
        
        if observed_verbs:
            for mech_type, typical_verbs in self.mechanism_sequences.items():
                # Count overlap
                overlap = len(observed_verbs.intersection(set(typical_verbs)))
                total = len(typical_verbs)
                
                if total > 0:
                    # Likelihood: proportion of typical verbs observed with Laplace smoothing
                    likelihood = (overlap + laplace_smooth) / (total + 2 * laplace_smooth)
                    
                    # Bayesian update
                    posterior[mech_type] *= likelihood
        
        # Update based on entity-activity
        if observations.get('entity_activity'):
            verb = observations['entity_activity'].get('verb_lemma', '')
            for mech_type, typical_verbs in self.mechanism_sequences.items():
                if verb in typical_verbs:
                    posterior[mech_type] *= 1.5
        
        # Normalize
        total = sum(posterior.values())
        if total > 0:
            posterior = {k: v / total for k, v in posterior.items()}
        
        return posterior
    
    def _infer_activity_sequence(self, observations: Dict[str, Any],
                                 mechanism_type_posterior: Dict[str, float]) -> Dict[str, Any]:
        """Infer activity sequence parameters"""
        # Get most likely mechanism type
        best_type = max(mechanism_type_posterior.items(), key=lambda x: x[1])[0]
        expected_sequence = self.mechanism_sequences.get(best_type, [])
        
        observed_verbs = observations.get('verbs', [])
        
        # Calculate transition probabilities (simplified Markov chain)
        transitions = {}
        for i in range(len(expected_sequence) - 1):
            current = expected_sequence[i]
            next_verb = expected_sequence[i + 1]
            
            # Check if transition is observed
            if current in observed_verbs and next_verb in observed_verbs:
                transitions[(current, next_verb)] = 0.85
            else:
                transitions[(current, next_verb)] = 0.40
        
        return {
            'expected_sequence': expected_sequence,
            'observed_verbs': observed_verbs,
            'transition_probabilities': transitions,
            'sequence_completeness': len(set(observed_verbs) & set(expected_sequence)) / max(len(expected_sequence), 1)
        }
    
    def _calculate_coherence_factor(self, node: MetaNode, 
                                   observations: Dict[str, Any],
                                   all_nodes: Dict[str, MetaNode]) -> float:
        """Calculate mechanism coherence score"""
        coherence = 0.0
        weights = []
        
        # Factor 1: Entity-Activity presence
        if observations.get('entity_activity'):
            coherence += 0.30
            weights.append(0.30)
        
        # Factor 2: Budget consistency
        if observations.get('budget'):
            coherence += 0.20
            weights.append(0.20)
        
        # Factor 3: Verb sequence completeness
        seq_info = observations.get('verbs', [])
        if seq_info:
            verb_score = min(len(seq_info) / 4.0, 1.0)  # Expect ~4 verbs
            coherence += verb_score * 0.25
            weights.append(0.25)
        
        # Factor 4: Entity presence
        if observations.get('entities'):
            coherence += 0.15
            weights.append(0.15)
        
        # Factor 5: Context richness
        snippets = observations.get('context_snippets', [])
        if snippets:
            coherence += 0.10
            weights.append(0.10)
        
        # Normalize by actual weights used
        if weights:
            coherence = coherence / sum(weights) if sum(weights) > 0 else 0.0
        
        return coherence
    
    def _test_sufficiency(self, node: MetaNode, 
                         observations: Dict[str, Any]) -> Dict[str, Any]:
        """Test if mechanism is sufficient to produce the outcome"""
        # Check if entity has capability
        has_entity = observations.get('entity_activity') is not None
        
        # Check if activities are present
        has_activities = len(observations.get('verbs', [])) >= 2
        
        # Check if resources are allocated
        has_resources = observations.get('budget') is not None
        
        sufficiency_score = (
            (0.4 if has_entity else 0.0) +
            (0.4 if has_activities else 0.0) +
            (0.2 if has_resources else 0.0)
        )
        
        return {
            'score': sufficiency_score,
            'is_sufficient': sufficiency_score >= 0.6,
            'components': {
                'entity': has_entity,
                'activities': has_activities,
                'resources': has_resources
            }
        }
    
    def _test_necessity(self, node: MetaNode, 
                       observations: Dict[str, Any]) -> Dict[str, Any]:
        """
        AUDIT POINT 2.2: Mechanism Necessity Hoop Test
        
        Test if mechanism is necessary by checking documented components:
        - Entity (responsable)
        - Activity (verb lemma sequence)
        - Budget (presupuesto asignado)
        
        Implements Beach 2017 Hoop Tests for necessity verification.
        Per Falleti & Lynch 2009, Bayesian-deterministic hybrid boosts mechanism depth.
        
        Returns:
            Dict with 'is_necessary', 'missing_components', and remediation text
        """
        # F1.2: Use refactored NecessitySufficiencyTester if available
        if self.bayesian_adapter and self.bayesian_adapter.necessity_tester:
            try:
                return self.bayesian_adapter.test_necessity_from_observations(
                    node.id,
                    observations
                )
            except Exception as e:
                self.logger.warning(f"Error en tester refactorizado: {e}, usando legacy")
        
        # AUDIT POINT 2.2: Enhanced necessity test with documented components
        missing_components = []
        
        # 1. Check Entity documentation
        entities = observations.get('entities', [])
        entity_activity = observations.get('entity_activity')
        
        if not entity_activity or not entity_activity.get('entity'):
            missing_components.append('entity')
        else:
            # Verify unique entity (not multiple conflicting entities)
            unique_entity = len(set(entities)) == 1 if entities else False
            if not unique_entity and len(entities) > 1:
                missing_components.append('unique_entity')
        
        # 2. Check Activity documentation (verb lemma sequence)
        verbs = observations.get('verbs', [])
        if not verbs or len(verbs) < 1:
            missing_components.append('activity')
        else:
            # Check for specific action verbs (not just generic ones)
            specific_verbs = [v for v in verbs if v in [
                'implementar', 'ejecutar', 'realizar', 'desarrollar',
                'construir', 'diseñar', 'planificar', 'coordinar',
                'gestionar', 'supervisar', 'controlar', 'auditar'
            ]]
            if not specific_verbs:
                missing_components.append('specific_activity')
        
        # 3. Check Budget documentation
        budget = observations.get('budget')
        if budget is None or budget <= 0:
            missing_components.append('budget')
        
        # Calculate necessity score
        # All three components must be present for necessity=True
        is_necessary = len(missing_components) == 0
        
        # Calculate partial score for reporting
        max_components = 3  # entity, activity, budget
        present_components = max_components - len([c for c in missing_components if c in ['entity', 'activity', 'budget']])
        necessity_score = present_components / max_components
        
        result = {
            'score': necessity_score,
            'is_necessary': is_necessary,
            'missing_components': missing_components,
            'alternatives_likely': not is_necessary,
            'hoop_test_passed': is_necessary
        }
        
        # Add remediation text if test fails
        if not is_necessary:
            result['remediation'] = self._generate_necessity_remediation(node.id, missing_components)
        
        return result
    
    def _generate_necessity_remediation(self, node_id: str, missing_components: List[str]) -> str:
        """Generate remediation text for failed necessity test"""
        component_descriptions = {
            'entity': 'entidad responsable claramente identificada',
            'unique_entity': 'una única entidad responsable (múltiples entidades detectadas)',
            'activity': 'secuencia de actividades documentada',
            'specific_activity': 'actividades específicas (no genéricas)',
            'budget': 'presupuesto asignado y cuantificado'
        }
        
        missing_desc = ', '.join([component_descriptions.get(c, c) for c in missing_components])
        
        return (
            f"Mecanismo para {node_id} falla Hoop Test de necesidad (D6-Q2). "
            f"Componentes faltantes: {missing_desc}. "
            f"Se requiere documentar estos componentes necesarios para validar "
            f"la cadena causal según Beach 2017."
        )
    
    def _quantify_uncertainty(self, mechanism_type_posterior: Dict[str, float],
                             sequence_posterior: Dict[str, Any],
                             coherence_score: float) -> Dict[str, float]:
        """Quantify epistemic uncertainty"""
        # Entropy of mechanism type distribution
        mech_probs = list(mechanism_type_posterior.values())
        if mech_probs:
            mech_entropy = -sum(p * np.log(p + 1e-10) for p in mech_probs if p > 0)
            max_entropy = np.log(len(mech_probs))
            mech_uncertainty = mech_entropy / max_entropy if max_entropy > 0 else 1.0
        else:
            mech_uncertainty = 1.0
        
        # Sequence completeness uncertainty
        seq_completeness = sequence_posterior.get('sequence_completeness', 0.0)
        seq_uncertainty = 1.0 - seq_completeness
        
        # Coherence uncertainty
        coherence_uncertainty = 1.0 - coherence_score
        
        # Combined uncertainty
        total_uncertainty = (
            mech_uncertainty * 0.4 +
            seq_uncertainty * 0.3 +
            coherence_uncertainty * 0.3
        )
        
        return {
            'total': total_uncertainty,
            'mechanism_type': mech_uncertainty,
            'sequence': seq_uncertainty,
            'coherence': coherence_uncertainty
        }
    
    def _detect_gaps(self, node: MetaNode, observations: Dict[str, Any],
                    uncertainty: Dict[str, float]) -> List[Dict[str, str]]:
        """Detect documentation gaps based on uncertainty"""
        gaps = []
        
        # High total uncertainty
        if uncertainty['total'] > 0.6:
            gaps.append({
                'type': 'high_uncertainty',
                'severity': 'high',
                'message': f"Mecanismo para {node.id} tiene alta incertidumbre ({uncertainty['total']:.2f})",
                'suggestion': "Se requiere más documentación sobre el mecanismo causal"
            })
        
        # Missing entity
        if not observations.get('entity_activity'):
            gaps.append({
                'type': 'missing_entity',
                'severity': 'high',
                'message': f"No se especifica entidad responsable para {node.id}",
                'suggestion': "Especificar qué entidad ejecutará las actividades"
            })
        
        # Insufficient activities
        if len(observations.get('verbs', [])) < 2:
            gaps.append({
                'type': 'insufficient_activities',
                'severity': 'medium',
                'message': f"Pocas actividades documentadas para {node.id}",
                'suggestion': "Detallar las actividades necesarias para lograr el producto"
            })
        
        # Missing budget
        if not observations.get('budget'):
            gaps.append({
                'type': 'missing_budget',
                'severity': 'medium',
                'message': f"Sin asignación presupuestaria para {node.id}",
                'suggestion': "Asignar recursos financieros al producto"
            })
        
        return gaps


class CausalInferenceSetup:
    """Prepare model for causal inference"""
    
    def __init__(self, config: ConfigLoader) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.goal_classification = config.get('lexicons.goal_classification', {})
        self.admin_keywords = config.get('lexicons.administrative_keywords', [])
        self.contextual_factors = config.get('lexicons.contextual_factors', [])
    
    def classify_goal_dynamics(self, nodes: Dict[str, MetaNode]) -> None:
        """Classify dynamics for each goal"""
        for node in nodes.values():
            text_lower = node.text.lower()
            
            for keyword, dynamics in self.goal_classification.items():
                if keyword in text_lower:
                    node.dynamics = cast(DynamicsType, dynamics)
                    self.logger.debug(f"Meta {node.id} clasificada como {node.dynamics}")
                    break
    
    def assign_probative_value(self, nodes: Dict[str, MetaNode]) -> None:
        """Assign probative test types to nodes"""
        # Import INDICATOR_STRUCTURE from financiero_viabilidad_tablas
        try:
            from financiero_viabilidad_tablas import ColombianMunicipalContext
            indicator_structure = ColombianMunicipalContext.INDICATOR_STRUCTURE
        except ImportError:
            indicator_structure = {
                'resultado': ['línea_base', 'meta', 'año_base', 'año_meta', 'fuente', 'responsable'],
                'producto': ['indicador', 'fórmula', 'unidad_medida', 'línea_base', 'meta', 'periodicidad'],
                'gestión': ['eficacia', 'eficiencia', 'economía', 'costo_beneficio']
            }
        
        for node in nodes.values():
            text_lower = node.text.lower()
            
            # Cross-reference with INDICATOR_STRUCTURE to classify critical requirements
            # as Hoop Tests or Smoking Guns
            required_fields = indicator_structure.get(node.type, [])
            
            # Check if node has all critical DNP requirements (D3-Q1 indicators)
            has_linea_base = bool(node.baseline and str(node.baseline).upper() not in ['ND', 'POR DEFINIR', 'N/A', 'NONE'])
            has_meta = bool(node.target and str(node.target).upper() not in ['ND', 'POR DEFINIR', 'N/A', 'NONE'])
            has_fuente = 'fuente' in text_lower or 'fuente de información' in text_lower
            
            # Perfect Hoop Test: Missing any critical requirement = total hypothesis failure
            # This applies to producto nodes with D3-Q1 indicators
            if node.type == 'producto':
                if has_linea_base and has_meta and has_fuente:
                    # Perfect indicators trigger Hoop Test classification
                    node.test_type = 'hoop_test'
                    self.logger.debug(f"Meta {node.id} classified as hoop_test (perfect D3-Q1 compliance)")
                elif not has_linea_base or not has_meta:
                    # Missing critical requirements - still Hoop Test but will fail
                    node.test_type = 'hoop_test'
                    node.audit_flags.append('hoop_test_failure')
                    self.logger.warning(f"Meta {node.id} FAILS hoop_test (missing D3-Q1 critical fields)")
                else:
                    node.test_type = 'straw_in_wind'
            # Check for administrative/regulatory nature (Hoop Test)
            elif any(keyword in text_lower for keyword in self.admin_keywords):
                node.test_type = 'hoop_test'
            # Check for highly specific outcomes (Smoking Gun)
            elif node.type == 'resultado' and node.target and node.baseline:
                try:
                    float(str(node.target).replace(',', '').replace('%', ''))
                    # Smoking Gun: rare, highly specific evidence with strong inferential power
                    node.test_type = 'smoking_gun'
                except (ValueError, TypeError):
                    node.test_type = 'straw_in_wind'
            # Double decisive for critical impact goals
            elif node.type == 'impacto' and node.rigor_status == 'fuerte':
                node.test_type = 'doubly_decisive'
            else:
                node.test_type = 'straw_in_wind'
            
            self.logger.debug(f"Meta {node.id} asignada test type: {node.test_type}")
    
    def identify_failure_points(self, graph: nx.DiGraph, text: str) -> Set[str]:
        """Identify single points of failure in causal chain
        
        Harmonic Front 3 - Enhancement 2: Contextual Failure Point Detection
        Expands risk_pattern to explicitly include localized contextual factors from rubrics:
        - restricciones territoriales
        - patrones culturales machistas
        - limitación normativa
        
        For D6-Q5 (Enfoque Diferencial/Restricciones): Excelente requires ≥3 distinct
        contextual factors correctly mapped to nodes, satisfying enfoque_diferencial
        and analisis_contextual criteria.
        """
        failure_points = set()
        
        # Find nodes with high out-degree (many dependencies)
        for node_id in graph.nodes():
            out_degree = graph.out_degree(node_id)
            node_type = graph.nodes[node_id].get('type')
            
            if node_type == 'producto' and out_degree >= 3:
                failure_points.add(node_id)
                self.logger.warning(f"Punto único de falla identificado: {node_id} "
                                  f"(grado de salida: {out_degree})")
        
        # HARMONIC FRONT 3 - Enhancement 2: Expand contextual factors
        # Add specific rubric factors for D6-Q5 compliance
        extended_contextual_factors = list(self.contextual_factors) + [
            'restricciones territoriales',
            'restricción territorial', 
            'limitación territorial',
            'patrones culturales machistas',
            'machismo',
            'inequidad de género',
            'violencia de género',
            'limitación normativa',
            'limitación legal',
            'restricción legal',
            'barrera institucional',
            'restricción presupuestal',
            'ausencia de capacidad técnica',
            'baja capacidad institucional',
            'conflicto armado',
            'desplazamiento forzado',
            'población dispersa',
            'ruralidad dispersa',
            'acceso vial limitado',
            'conectividad deficiente'
        ]
        
        # Extract contextual risks from text
        risk_pattern = '|'.join(re.escape(factor) for factor in extended_contextual_factors)
        risk_regex = re.compile(rf'\b({risk_pattern})\b', re.IGNORECASE)
        
        # Track distinct contextual factors for D6-Q5 quality criteria
        contextual_factors_detected = set()
        node_contextual_map = defaultdict(set)
        
        # Find risk mentions and associate with nodes
        for match in risk_regex.finditer(text):
            risk_text = match.group()
            contextual_factors_detected.add(risk_text.lower())
            
            context_start = max(0, match.start() - 200)
            context_end = min(len(text), match.end() + 200)
            context = text[context_start:context_end]
            
            # Try to find node mentions in risk context
            for node_id in graph.nodes():
                if node_id in context:
                    failure_points.add(node_id)
                    if 'contextual_risks' not in graph.nodes[node_id]:
                        graph.nodes[node_id]['contextual_risks'] = []
                    graph.nodes[node_id]['contextual_risks'].append(risk_text)
                    node_contextual_map[node_id].add(risk_text.lower())
        
        # D6-Q5 quality criteria assessment
        distinct_factors_count = len(contextual_factors_detected)
        d6_q5_quality = 'insuficiente'
        if distinct_factors_count >= 3:
            d6_q5_quality = 'excelente'
        elif distinct_factors_count >= 2:
            d6_q5_quality = 'bueno'
        elif distinct_factors_count >= 1:
            d6_q5_quality = 'aceptable'
        
        # Store D6-Q5 metrics in graph attributes
        graph.graph['d6_q5_contextual_factors'] = list(contextual_factors_detected)
        graph.graph['d6_q5_distinct_count'] = distinct_factors_count
        graph.graph['d6_q5_quality'] = d6_q5_quality
        graph.graph['d6_q5_node_mapping'] = dict(node_contextual_map)
        
        self.logger.info(f"Puntos de falla identificados: {len(failure_points)}")
        self.logger.info(f"D6-Q5: {distinct_factors_count} factores contextuales distintos detectados - {d6_q5_quality}")
        
        return failure_points


class ReportingEngine:
    """Generate visualizations and reports"""
    
    def __init__(self, config: ConfigLoader, output_dir: Path) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_causal_diagram(self, graph: nx.DiGraph, policy_code: str) -> Path:
        """Generate causal diagram visualization"""
        dot = Dot(graph_type='digraph', rankdir='TB')
        dot.set_name(f'{policy_code}_causal_model')
        dot.set_node_defaults(
            shape='box',
            style='rounded,filled',
            fontname='Arial',
            fontsize='10'
        )
        dot.set_edge_defaults(
            fontsize='8',
            fontname='Arial'
        )
        
        # Add nodes with rigor coloring
        for node_id in graph.nodes():
            node_data = graph.nodes[node_id]
            
            # Determine color based on rigor status and audit flags
            rigor = node_data.get('rigor_status', 'sin_evaluar')
            audit_flags = node_data.get('audit_flags', [])
            financial = node_data.get('financial_allocation')
            
            if rigor == 'débil' or not financial:
                color = 'lightcoral'  # Red
            elif audit_flags:
                color = 'lightyellow'  # Yellow
            else:
                color = 'lightgreen'  # Green
            
            # Create label
            node_type = node_data.get('type', 'programa')
            text = node_data.get('text', '')[:80]
            label = f"{node_id}\\n[{node_type.upper()}]\\n{text}..."
            
            entity = node_data.get('responsible_entity')
            if entity:
                label += f"\\n👤 {entity[:30]}"
            
            if financial:
                label += f"\\n💰 ${financial:,.0f}"
            
            dot_node = Node(
                node_id,
                label=label,
                fillcolor=color
            )
            dot.add_node(dot_node)
        
        # Add edges with causal logic
        for source, target in graph.edges():
            edge_data = graph.edges[source, target]
            keyword = edge_data.get('keyword', '')
            strength = edge_data.get('strength', 0.5)
            
            # Determine edge style based on strength
            style = 'solid' if strength > 0.7 else 'dashed'
            
            dot_edge = Edge(
                source,
                target,
                label=keyword[:20],
                style=style
            )
            dot.add_edge(dot_edge)
        
        # Save files
        dot_path = self.output_dir / f"{policy_code}_causal_diagram.dot"
        png_path = self.output_dir / f"{policy_code}_causal_diagram.png"
        
        try:
            with open(dot_path, 'w', encoding='utf-8') as f:
                f.write(dot.to_string())
            self.logger.info(f"Diagrama DOT guardado en: {dot_path}")
            
            # Try to render PNG
            try:
                dot.write_png(str(png_path))
                self.logger.info(f"Diagrama PNG renderizado en: {png_path}")
            except Exception as e:
                self.logger.warning(f"No se pudo renderizar PNG (¿Graphviz instalado?): {e}")
        except Exception as e:
            self.logger.error(f"Error guardando diagrama: {e}")
        
        return png_path
    
    def generate_accountability_matrix(self, graph: nx.DiGraph, 
                                      policy_code: str) -> Path:
        """Generate accountability matrix in Markdown"""
        md_path = self.output_dir / f"{policy_code}_accountability_matrix.md"
        
        # Group by impact goals
        impact_goals = [n for n in graph.nodes() 
                       if graph.nodes[n].get('type') == 'impacto']
        
        content = [f"# Matriz de Responsabilidades - {policy_code}\n"]
        content.append(f"*Generado automáticamente por CDAF v2.0*\n")
        content.append("---\n\n")
        
        for impact in impact_goals:
            impact_data = graph.nodes[impact]
            content.append(f"## Meta de Impacto: {impact}\n")
            content.append(f"**Descripción:** {impact_data.get('text', 'N/A')}\n\n")
            
            # Find all predecessor chains
            predecessors = list(nx.ancestors(graph, impact))
            
            if predecessors:
                content.append("| Meta | Tipo | Entidad Responsable | Actividad Clave | Presupuesto |\n")
                content.append("|------|------|---------------------|-----------------|-------------|\n")
                
                for pred in predecessors:
                    pred_data = graph.nodes[pred]
                    meta_type = pred_data.get('type', 'N/A')
                    entity = pred_data.get('responsible_entity', 'No asignado')
                    
                    ea = pred_data.get('entity_activity')
                    activity = 'N/A'
                    if ea and isinstance(ea, dict):
                        activity = ea.get('activity', 'N/A')
                    
                    budget = pred_data.get('financial_allocation')
                    budget_str = f"${budget:,.0f}" if budget else "Sin presupuesto"
                    
                    content.append(f"| {pred} | {meta_type} | {entity} | {activity} | {budget_str} |\n")
                
                content.append("\n")
            else:
                content.append("*No se encontraron metas intermedias.*\n\n")
        
        content.append("\n---\n")
        content.append("### Leyenda\n")
        content.append("- **Meta de Impacto:** Resultado final esperado\n")
        content.append("- **Meta de Resultado:** Cambio intermedio observable\n")
        content.append("- **Meta de Producto:** Entrega tangible del programa\n")
        
        try:
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(''.join(content))
            self.logger.info(f"Matriz de responsabilidades guardada en: {md_path}")
        except Exception as e:
            self.logger.error(f"Error guardando matriz de responsabilidades: {e}")
        
        return md_path
    
    def generate_confidence_report(self, 
                                   nodes: Dict[str, MetaNode],
                                   graph: nx.DiGraph,
                                   causal_chains: List[CausalLink],
                                   audit_results: Dict[str, AuditResult],
                                   financial_auditor: FinancialAuditor,
                                   sequence_warnings: List[str],
                                   policy_code: str) -> Path:
        """Generate extraction confidence report"""
        json_path = self.output_dir / f"{policy_code}_extraction_confidence_report.json"
        
        # Calculate metrics
        total_metas = len(nodes)
        
        metas_with_ea = sum(1 for n in nodes.values() if n.entity_activity)
        metas_with_ea_pct = (metas_with_ea / total_metas * 100) if total_metas > 0 else 0
        
        enlaces_with_logic = sum(1 for link in causal_chains if link.get('logic'))
        total_edges = graph.number_of_edges()
        enlaces_with_logic_pct = (enlaces_with_logic / total_edges * 100) if total_edges > 0 else 0
        
        metas_passed_audit = sum(1 for r in audit_results.values() if r['passed'])
        metas_with_traceability_pct = (metas_passed_audit / total_metas * 100) if total_metas > 0 else 0
        
        metas_with_financial = sum(1 for n in nodes.values() if n.financial_allocation)
        metas_with_financial_pct = (metas_with_financial / total_metas * 100) if total_metas > 0 else 0
        
        # Node type distribution
        type_distribution = defaultdict(int)
        for node in nodes.values():
            type_distribution[node.type] += 1
        
        # Rigor distribution
        rigor_distribution = defaultdict(int)
        for node in nodes.values():
            rigor_distribution[node.rigor_status] += 1
        
        report = {
            "metadata": {
                "policy_code": policy_code,
                "framework_version": "2.0.0",
                "total_nodes": total_metas,
                "total_edges": total_edges
            },
            "extraction_metrics": {
                "total_metas_identificadas": total_metas,
                "metas_con_EA_extraido": metas_with_ea,
                "metas_con_EA_extraido_pct": round(metas_with_ea_pct, 2),
                "enlaces_con_logica_causal": enlaces_with_logic,
                "enlaces_con_logica_causal_pct": round(enlaces_with_logic_pct, 2),
                "metas_con_trazabilidad_evidencia": metas_passed_audit,
                "metas_con_trazabilidad_evidencia_pct": round(metas_with_traceability_pct, 2),
                "metas_con_trazabilidad_financiera": metas_with_financial,
                "metas_con_trazabilidad_financiera_pct": round(metas_with_financial_pct, 2)
            },
            "financial_audit": {
                "tablas_financieras_parseadas_exitosamente": financial_auditor.successful_parses,
                "tablas_financieras_fallidas": financial_auditor.failed_parses,
                "asignaciones_presupuestarias_rastreadas": len(financial_auditor.financial_data)
            },
            "sequence_audit": {
                "alertas_secuencia_logica": len(sequence_warnings),
                "detalles": sequence_warnings
            },
            "type_distribution": dict(type_distribution),
            "rigor_distribution": dict(rigor_distribution),
            "audit_summary": {
                "total_audited": len(audit_results),
                "passed": sum(1 for r in audit_results.values() if r['passed']),
                "failed": sum(1 for r in audit_results.values() if not r['passed']),
                "total_warnings": sum(len(r['warnings']) for r in audit_results.values()),
                "total_errors": sum(len(r['errors']) for r in audit_results.values())
            },
            "quality_score": self._calculate_quality_score(
                metas_with_traceability_pct,
                metas_with_financial_pct,
                enlaces_with_logic_pct,
                metas_with_ea_pct
            )
        }
        
        try:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Reporte de confianza guardado en: {json_path}")
        except Exception as e:
            self.logger.error(f"Error guardando reporte de confianza: {e}")
        
        return json_path
    
    def _calculate_quality_score(self, traceability: float, financial: float, 
                                 logic: float, ea: float) -> float:
        """Calculate overall quality score (0-100)"""
        weights = {'traceability': 0.35, 'financial': 0.25, 'logic': 0.25, 'ea': 0.15}
        score = (traceability * weights['traceability'] +
                financial * weights['financial'] +
                logic * weights['logic'] +
                ea * weights['ea'])
        return round(score, 2)
    
    def generate_causal_model_json(self, graph: nx.DiGraph, nodes: Dict[str, MetaNode],
                                  policy_code: str) -> Path:
        """Generate structured JSON export of causal model"""
        json_path = self.output_dir / f"{policy_code}_causal_model.json"
        
        # Prepare node data
        nodes_data = {}
        for node_id, node in nodes.items():
            node_dict = asdict(node)
            # Convert NamedTuple to dict
            if node.entity_activity:
                node_dict['entity_activity'] = node.entity_activity._asdict()
            nodes_data[node_id] = node_dict
        
        # Prepare edge data
        edges_data = []
        for source, target in graph.edges():
            edge_dict = {
                'source': source,
                'target': target,
                **graph.edges[source, target]
            }
            edges_data.append(edge_dict)
        
        model_data = {
            "policy_code": policy_code,
            "framework_version": "2.0.0",
            "nodes": nodes_data,
            "edges": edges_data,
            "statistics": {
                "total_nodes": len(nodes_data),
                "total_edges": len(edges_data),
                "node_types": {
                    node_type: sum(1 for n in nodes.values() if n.type == node_type)
                    for node_type in ['programa', 'producto', 'resultado', 'impacto']
                }
            }
        }
        
        try:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(model_data, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Modelo causal JSON guardado en: {json_path}")
        except Exception as e:
            self.logger.error(f"Error guardando modelo causal: {e}")
        
        return json_path


class CDAFFramework:
    """Main orchestrator for the CDAF pipeline"""
    
    def __init__(self, config_path: Path, output_dir: Path, log_level: str = "INFO") -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.setLevel(getattr(logging, log_level.upper()))
        
        # Initialize components
        self.config = ConfigLoader(config_path)
        self.output_dir = output_dir
        
        # Initialize retry handler for external dependencies
        try:
            from retry_handler import get_retry_handler, DependencyType
            self.retry_handler = get_retry_handler()
            retry_enabled = True
        except ImportError:
            self.logger.warning("RetryHandler no disponible, funcionando sin retry logic")
            self.retry_handler = None
            retry_enabled = False
        
        # Load spaCy model with retry logic
        if retry_enabled and self.retry_handler:
            @self.retry_handler.with_retry(
                DependencyType.SPACY_MODEL,
                operation_name="load_spacy_model",
                exceptions=(OSError, IOError, ImportError)
            )
            def load_spacy_with_retry():
                try:
                    nlp = spacy.load("es_core_news_lg")
                    self.logger.info("Modelo spaCy cargado: es_core_news_lg")
                    return nlp
                except OSError:
                    self.logger.warning("Modelo es_core_news_lg no encontrado. Intentando es_core_news_sm...")
                    nlp = spacy.load("es_core_news_sm")
                    return nlp
            
            try:
                self.nlp = load_spacy_with_retry()
            except OSError:
                self.logger.error("No se encontró ningún modelo de spaCy en español. "
                                "Ejecute: python -m spacy download es_core_news_lg")
                sys.exit(1)
        else:
            # Fallback to original logic without retry
            try:
                self.nlp = spacy.load("es_core_news_lg")
                self.logger.info("Modelo spaCy cargado: es_core_news_lg")
            except OSError:
                self.logger.warning("Modelo es_core_news_lg no encontrado. Intentando es_core_news_sm...")
                try:
                    self.nlp = spacy.load("es_core_news_sm")
                except OSError:
                    self.logger.error("No se encontró ningún modelo de spaCy en español. "
                                    "Ejecute: python -m spacy download es_core_news_lg")
                    sys.exit(1)
        
        # Initialize modules (pass retry_handler to PDF processor)
        self.pdf_processor = PDFProcessor(self.config, retry_handler=self.retry_handler if retry_enabled else None)
        self.causal_extractor = CausalExtractor(self.config, self.nlp)
        self.mechanism_extractor = MechanismPartExtractor(self.config, self.nlp)
        self.bayesian_mechanism = BayesianMechanismInference(self.config, self.nlp)
        self.financial_auditor = FinancialAuditor(self.config)
        self.op_auditor = OperationalizationAuditor(self.config)
        self.inference_setup = CausalInferenceSetup(self.config)
        self.reporting_engine = ReportingEngine(self.config, output_dir)
        
        # Initialize DNP validator if available
        self.dnp_validator = None
        if DNP_AVAILABLE:
            self.dnp_validator = ValidadorDNP(es_municipio_pdet=False)  # Can be configured
            self.logger.info("Validador DNP inicializado")
    
    def process_document(self, pdf_path: Path, policy_code: str) -> bool:
        """Main processing pipeline"""
        self.logger.info(f"Iniciando procesamiento de documento: {pdf_path}")
        
        try:
            # Step 1: Load and extract PDF
            if not self.pdf_processor.load_document(pdf_path):
                return False
