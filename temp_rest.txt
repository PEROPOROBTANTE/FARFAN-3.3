class MechanismPartExtractor:
    """Extract Entity-Activity pairs for mechanism parts"""
    
    def __init__(self, config: ConfigLoader, nlp_model: spacy.Language) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.nlp = nlp_model
        self.entity_aliases = config.get('entity_aliases', {})
    
    def extract_entity_activity(self, text: str) -> Optional[EntityActivity]:
        """Extract Entity-Activity tuple from text"""
        doc = self.nlp(text)
        
        # Find main verb (activity)
        main_verb = None
        for token in doc:
            if token.pos_ == 'VERB' and token.dep_ in ['ROOT', 'ccomp']:
                main_verb = token
                break
        
        if not main_verb:
            return None
        
        # Find subject entity
        entity = None
        for child in main_verb.children:
            if child.dep_ in ['nsubj', 'nsubjpass']:
                entity = self._normalize_entity(child.text)
                break
        
        if not entity:
            # Try to find entity from NER
            for ent in doc.ents:
                if ent.label_ in ['ORG', 'PER']:
                    entity = self._normalize_entity(ent.text)
                    break
        
        if entity and main_verb:
            return EntityActivity(
                entity=entity,
                activity=main_verb.text,
                verb_lemma=main_verb.lemma_,
                confidence=0.85
            )
        
        return None
    
    def _normalize_entity(self, entity: str) -> str:
        """Normalize entity name using aliases"""
        entity_upper = entity.upper().strip()
        return self.entity_aliases.get(entity_upper, entity)


class FinancialAuditor:
    """Financial traceability and auditing"""
    
    def __init__(self, config: ConfigLoader) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.financial_data: Dict[str, Dict[str, float]] = {}
        self.unit_costs: Dict[str, float] = {}
        self.successful_parses = 0
        self.failed_parses = 0
        self.d3_q3_analysis: Dict[str, Any] = {}  # Harmonic Front 3 - D3-Q3 metrics
    
    def trace_financial_allocation(self, tables: List[pd.DataFrame], 
                                  nodes: Dict[str, MetaNode],
                                  graph: Optional[nx.DiGraph] = None) -> Dict[str, float]:
        """Trace financial allocations to programs/goals
        
        Harmonic Front 3 - Enhancement 5: Single-Case Counterfactual Budget Check
        Incorporates logic from single-case counterfactuals to test minimal sufficiency.
        For D3-Q3 (Traceability/Resources): checks if resource X (BPIN code) were removed,
        would the mechanism (Product) still execute? Only boosts budget traceability score
        if allocation is tied to a specific project.
        """
        for i, table in enumerate(tables):
            try:
                self.logger.info(f"Procesando tabla financiera {i+1}/{len(tables)}")
                self._process_financial_table(table, nodes)
                self.successful_parses += 1
            except Exception as e:
                self.logger.error(f"Error procesando tabla financiera {i+1}: {e}")
                self.failed_parses += 1
                continue
        
        # HARMONIC FRONT 3 - Enhancement 5: Counterfactual sufficiency check
        if graph is not None:
            self._perform_counterfactual_budget_check(nodes, graph)
        
        self.logger.info(f"Asignaciones financieras trazadas: {len(self.financial_data)}")
        self.logger.info(f"Tablas parseadas exitosamente: {self.successful_parses}, "
                        f"Fallidas: {self.failed_parses}")
        return self.unit_costs
    
    def _process_financial_table(self, table: pd.DataFrame, 
                                nodes: Dict[str, MetaNode]) -> None:
        """Process a single financial table"""
        # Try to identify relevant columns
        amount_pattern = re.compile(
            self.config.get('patterns.financial_headers', r'PRESUPUESTO|VALOR|MONTO'),
            re.IGNORECASE
        )
        program_pattern = re.compile(r'PROGRAMA|META|CÃ“DIGO', re.IGNORECASE)
        
        amount_col = None
        program_col = None
        
        # Search in column names
        for col in table.columns:
            col_str = str(col)
            if amount_pattern.search(col_str) and not amount_col:
                amount_col = col
            if program_pattern.search(col_str) and not program_col:
                program_col = col
        
        # If not found in column names, search in first row
        if not amount_col or not program_col:
            first_row = table.iloc[0]
            for i, val in enumerate(first_row):
                val_str = str(val)
                if amount_pattern.search(val_str) and not amount_col:
                    amount_col = i
                    table.columns = table.iloc[0]
                    table = table[1:]
                if program_pattern.search(val_str) and not program_col:
                    program_col = i
                    table.columns = table.iloc[0]
                    table = table[1:]
        
        if amount_col is None or program_col is None:
            self.logger.warning("No se encontraron columnas financieras relevantes")
            return
        
        for _, row in table.iterrows():
            try:
                program_id = str(row[program_col]).strip().upper()
                amount = self._parse_amount(row[amount_col])
                
                if amount and program_id:
                    matched_node = self._match_program_to_node(program_id, nodes)
                    if matched_node:
                        self.financial_data[matched_node] = {
                            'allocation': amount,
                            'source': 'budget_table'
                        }
                        
                        # Update node
                        nodes[matched_node].financial_allocation = amount
                        
                        # Calculate unit cost if possible
                        node = nodes.get(matched_node)
                        if node and node.target:
                            try:
                                target_val = float(str(node.target).replace(',', '').replace('%', ''))
                                if target_val > 0:
                                    unit_cost = amount / target_val
                                    self.unit_costs[matched_node] = unit_cost
                                    nodes[matched_node].unit_cost = unit_cost
                            except (ValueError, TypeError):
                                pass
                                
            except Exception as e:
                self.logger.debug(f"Error procesando fila financiera: {e}")
                continue
    
    def _parse_amount(self, value: Any) -> Optional[float]:
        """Parse monetary amount from various formats"""
        if pd.isna(value):
            return None
        
        try:
            clean_value = str(value).replace('$', '').replace(',', '').replace(' ', '').replace('.', '')
            # Handle millions/thousands notation
            if 'M' in clean_value.upper() or 'MILLONES' in clean_value.upper():
                clean_value = clean_value.upper().replace('M', '').replace('ILLONES', '')
                return float(clean_value) * 1_000_000
            return float(clean_value)
        except (ValueError, TypeError):
            return None
    
    def _match_program_to_node(self, program_id: str, 
                               nodes: Dict[str, MetaNode]) -> Optional[str]:
        """Match program ID to existing node using fuzzy matching
        
        Enhanced for D1-Q3 / D3-Q3 Financial Traceability:
        - Implements confidence penalty if fuzzy match ratio < 100
        - Reduces node.financial_allocation confidence by 15% for imperfect matches
        - Tracks match quality for overall financial traceability scoring
        """
        if program_id in nodes:
            # Perfect match - no penalty
            return program_id
        
        # Try fuzzy matching
        best_match = process.extractOne(
            program_id,
            nodes.keys(),
            scorer=fuzz.ratio,
            score_cutoff=80
        )
        
        if best_match:
            matched_node_id = best_match[0]
            match_ratio = best_match[1]
            
            # D1-Q3 / D3-Q3: Apply confidence penalty for non-perfect matches
            if match_ratio < 100:
                penalty_factor = 0.85  # 15% reduction as specified
                node = nodes[matched_node_id]
                
                # Track original allocation before penalty
                if not hasattr(node, '_original_financial_allocation'):
                    node._original_financial_allocation = node.financial_allocation
                
                # Apply penalty to financial allocation confidence
                if node.financial_allocation:
                    penalized_allocation = node.financial_allocation * penalty_factor
                    self.logger.debug(
                        f"Fuzzy match penalty applied to {matched_node_id}: "
                        f"ratio={match_ratio}, penalty={penalty_factor:.2f}, "
                        f"allocation {node.financial_allocation:.0f} -> {penalized_allocation:.0f}"
                    )
                    node.financial_allocation = penalized_allocation
                
                # Store match confidence for D1-Q3 / D3-Q3 scoring
                if not hasattr(node, 'financial_match_confidence'):
                    node.financial_match_confidence = match_ratio / 100.0
                else:
                    # Average if multiple matches
                    node.financial_match_confidence = (node.financial_match_confidence + match_ratio / 100.0) / 2
            
            return matched_node_id
        
        return None
    
    def _perform_counterfactual_budget_check(self, nodes: Dict[str, MetaNode], 
                                            graph: nx.DiGraph) -> None:
        """
        Harmonic Front 3 - Enhancement 5: Counterfactual Sufficiency Test for D3-Q3
        
        Tests minimal sufficiency: if resource X (BPIN code) were removed, would the
        mechanism (Product) still execute? Only boosts budget traceability score if
        allocation is tied to a specific project.
        
        For D3-Q3 (Traceability/Resources): ensures funding is necessary for the mechanism
        and prevents false positives from generic or disconnected budget entries.
        """
        d3_q3_scores = {}
        
        for node_id, node in nodes.items():
            if node.type != 'producto':
                continue
            
            # Check if node has financial allocation
            has_budget = node.financial_allocation is not None and node.financial_allocation > 0
            
            # Check if node has entity-activity (mechanism)
            has_mechanism = node.entity_activity is not None
            
            # Check if node has dependencies (successors in graph)
            successors = list(graph.successors(node_id)) if graph.has_node(node_id) else []
            has_dependencies = len(successors) > 0
            
            # Counterfactual test: Would mechanism still execute without this budget?
            # Check if there are alternative funding sources or generic allocations
            financial_source = self.financial_data.get(node_id, {}).get('source', 'unknown')
            is_specific_allocation = financial_source == 'budget_table'  # From specific table entry
            
            # Calculate counterfactual necessity score
            # High score = budget is necessary for execution
            # Low score = budget may be generic/disconnected
            necessity_score = 0.0
            
            if has_budget and has_mechanism:
                necessity_score += 0.40  # Budget + mechanism present
            
            if has_budget and has_dependencies:
                necessity_score += 0.30  # Budget supports downstream goals
            
            if is_specific_allocation:
                necessity_score += 0.30  # Specific allocation (not generic)
            
            # D3-Q3 quality criteria
            d3_q3_quality = 'insuficiente'
            if necessity_score >= 0.85:
                d3_q3_quality = 'excelente'
            elif necessity_score >= 0.70:
                d3_q3_quality = 'bueno'
            elif necessity_score >= 0.50:
                d3_q3_quality = 'aceptable'
            
            d3_q3_scores[node_id] = {
                'necessity_score': necessity_score,
                'd3_q3_quality': d3_q3_quality,
                'has_budget': has_budget,
                'has_mechanism': has_mechanism,
                'has_dependencies': has_dependencies,
                'is_specific_allocation': is_specific_allocation,
                'counterfactual_sufficient': necessity_score < 0.50,  # Would still execute without budget
                'budget_necessary': necessity_score >= 0.70  # Budget is necessary
            }
            
            # Store in node for later retrieval
            node.audit_flags = node.audit_flags or []
            if necessity_score < 0.50:
                node.audit_flags.append('budget_not_necessary')
                self.logger.warning(f"D3-Q3: {node_id} may execute without allocated budget (score={necessity_score:.2f})")
            elif necessity_score >= 0.85:
                node.audit_flags.append('budget_well_traced')
                self.logger.info(f"D3-Q3: {node_id} has well-traced, necessary budget (score={necessity_score:.2f})")
        
        # Store aggregate D3-Q3 metrics
        self.d3_q3_analysis = {
            'node_scores': d3_q3_scores,
            'total_products_analyzed': len(d3_q3_scores),
            'well_traced_count': sum(1 for s in d3_q3_scores.values() if s['d3_q3_quality'] == 'excelente'),
            'average_necessity_score': sum(s['necessity_score'] for s in d3_q3_scores.values()) / max(len(d3_q3_scores), 1)
        }
        
        self.logger.info(f"D3-Q3 Counterfactual Budget Check completed: "
                        f"{self.d3_q3_analysis['well_traced_count']}/{len(d3_q3_scores)} "
                        f"products with excellent traceability")
        if best_match:
            return best_match[0]
        
        return None


class OperationalizationAuditor:
    """Audit operationalization quality"""
    
    def __init__(self, config: ConfigLoader) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.verb_sequences = config.get('verb_sequences', {})
        self.audit_results: Dict[str, AuditResult] = {}
        self.sequence_warnings: List[str] = []
    
    def audit_evidence_traceability(self, nodes: Dict[str, MetaNode]) -> Dict[str, AuditResult]:
        """Audit evidence traceability for all nodes
        
        Enhanced with D3-Q1 Ficha TÃ©cnica validation:
        - Cross-checks baseline/target against extracted quantitative_claims
        - Verifies DNP INDICATOR_STRUCTURE compliance for producto nodes
        - Scores 'Excelente' only if â‰¥80% of productos pass full audit
        """
        # Import for quantitative claims extraction
        try:
            from contradiction_deteccion import PolicyContradictionDetectorV2
            has_detector = True
        except ImportError:
            has_detector = False
            self.logger.warning("PolicyContradictionDetectorV2 not available for quantitative claims validation")
        
        producto_nodes_count = 0
        producto_nodes_passed = 0
        
        for node_id, node in nodes.items():
            result: AuditResult = {
                'passed': True,
                'warnings': [],
                'errors': [],
                'recommendations': []
            }
            
            # Track producto nodes for D3-Q1 scoring
            if node.type == 'producto':
                producto_nodes_count += 1
            
            # Extract quantitative claims from node text if detector available
            quantitative_claims = []
            if has_detector:
                try:
                    # Create temporary detector instance
                    detector = PolicyContradictionDetectorV2(device='cpu')
                    quantitative_claims = detector._extract_structured_quantitative_claims(node.text)
                except Exception as e:
                    self.logger.debug(f"Could not extract quantitative claims: {e}")
            
            # Check baseline
            baseline_valid = False
            if not node.baseline or str(node.baseline).upper() in ['ND', 'POR DEFINIR', 'N/A', 'NONE']:
                result['errors'].append(f"LÃ­nea base no definida para {node_id}")
                result['passed'] = False
                node.rigor_status = 'dÃ©bil'
                node.audit_flags.append('sin_linea_base')
            else:
                baseline_valid = True
                # Cross-check baseline against quantitative claims (D3-Q1)
                if quantitative_claims:
                    baseline_in_claims = any(
                        claim.get('type') in ['indicator', 'target', 'percentage', 'beneficiaries'] 
                        for claim in quantitative_claims
                    )
                    if not baseline_in_claims:
                        result['warnings'].append(f"LÃ­nea base no verificada en claims cuantitativos para {node_id}")
            
            # Check target
            target_valid = False
            if not node.target or str(node.target).upper() in ['ND', 'POR DEFINIR', 'N/A', 'NONE']:
                result['errors'].append(f"Meta no definida para {node_id}")
                result['passed'] = False
                node.rigor_status = 'dÃ©bil'
                node.audit_flags.append('sin_meta')
            else:
                target_valid = True
                # Cross-check target against quantitative claims (D3-Q1)
                if quantitative_claims:
                    meta_in_claims = any(
                        claim.get('type') == 'target' or 'meta' in claim.get('context', '').lower()
                        for claim in quantitative_claims
                    )
                    if not meta_in_claims:
                        result['warnings'].append(f"Meta no verificada en claims cuantitativos para {node_id}")
            
            # D3-Q1 Ficha TÃ©cnica compliance check for producto nodes
            if node.type == 'producto':
                # Check if has all minimum DNP INDICATOR_STRUCTURE elements
                has_complete_ficha = (
                    baseline_valid and 
                    target_valid and 
                    'sin_linea_base' not in node.audit_flags and
                    'sin_meta' not in node.audit_flags
                )
                
                if has_complete_ficha and quantitative_claims:
                    # Node passes D3-Q1 compliance
                    producto_nodes_passed += 1
                    result['recommendations'].append(f"D3-Q1 Ficha TÃ©cnica completa para {node_id}")
                elif has_complete_ficha:
                    # Has baseline/target but no quantitative claims verification
                    producto_nodes_passed += 0.5  # Partial credit
                    result['warnings'].append(f"D3-Q1 parcial: Ficha bÃ¡sica sin verificaciÃ³n cuantitativa en {node_id}")
            
            # Check responsible entity
            if not node.responsible_entity:
                result['warnings'].append(f"Entidad responsable no identificada para {node_id}")
                node.audit_flags.append('sin_responsable')
            
            # Check financial traceability
            if not node.financial_allocation:
                result['warnings'].append(f"Sin trazabilidad financiera para {node_id}")
                node.audit_flags.append('sin_presupuesto')
            
            # Set rigor status if passed all checks
            if result['passed'] and len(result['warnings']) == 0:
                node.rigor_status = 'fuerte'
            
            self.audit_results[node_id] = result
        
        # Calculate D3-Q1 compliance score
        if producto_nodes_count > 0:
            d3_q1_compliance_pct = (producto_nodes_passed / producto_nodes_count) * 100
            self.logger.info(f"D3-Q1 Ficha TÃ©cnica Compliance: {d3_q1_compliance_pct:.1f}% "
                           f"({producto_nodes_passed}/{producto_nodes_count} productos)")
            
            if d3_q1_compliance_pct >= 80:
                self.logger.info("D3-Q1 Score: EXCELENTE (â‰¥80% productos con Ficha TÃ©cnica completa)")
            elif d3_q1_compliance_pct >= 60:
                self.logger.info("D3-Q1 Score: BUENO (60-80% compliance)")
            else:
                self.logger.warning("D3-Q1 Score: INSUFICIENTE (<60% compliance)")
        
        passed_count = sum(1 for r in self.audit_results.values() if r['passed'])
        self.logger.info(f"AuditorÃ­a de trazabilidad: {passed_count}/{len(nodes)} nodos aprobados")
        
        return self.audit_results
    
    def audit_sequence_logic(self, graph: nx.DiGraph) -> List[str]:
        """Audit logical sequence of activities"""
        warnings = []
        
        # Group nodes by program
        programs: Dict[str, List[str]] = defaultdict(list)
        for node_id in graph.nodes():
            node_data = graph.nodes[node_id]
            if node_data.get('type') == 'programa':
                for successor in graph.successors(node_id):
                    if graph.nodes[successor].get('type') == 'producto':
                        programs[node_id].append(successor)
        
        # Check sequence within each program
        for program_id, product_goals in programs.items():
            if len(product_goals) < 2:
                continue
            
            activities = []
            for goal_id in product_goals:
                node = graph.nodes[goal_id]
                ea = node.get('entity_activity')
                if ea and isinstance(ea, dict):
                    verb = ea.get('verb_lemma', '')
                    sequence_num = self.verb_sequences.get(verb, 999)
                    activities.append((goal_id, verb, sequence_num))
            
            # Check for sequence violations
            activities.sort(key=lambda x: x[2])
            for i in range(len(activities) - 1):
                if activities[i][2] > activities[i + 1][2]:
                    warning = (f"ViolaciÃ³n de secuencia en {program_id}: "
                             f"{activities[i][1]} ({activities[i][0]}) "
                             f"antes de {activities[i + 1][1]} ({activities[i + 1][0]})")
                    warnings.append(warning)
                    self.logger.warning(warning)
        
        self.sequence_warnings = warnings
        return warnings
    
    def bayesian_counterfactual_audit(self, nodes: Dict[str, MetaNode], 
                                     graph: nx.DiGraph,
                                     historical_data: Optional[Dict[str, Any]] = None,
                                     pdet_alignment: Optional[float] = None) -> Dict[str, Any]:
        """
        AGUJA III: El Auditor Contrafactual Bayesiano
        Perform counterfactual audit using Bayesian causal reasoning
        
        Harmonic Front 3: Enhanced to consume pdet_alignment scores for D4-Q5 and D5-Q4 integration
        """
        self.logger.info("Iniciando auditorÃ­a contrafactual Bayesiana...")
        
        # Build implicit Structural Causal Model (SCM)
        scm_dag = self._build_normative_dag()
        
        # Initialize historical priors
        if historical_data is None:
            historical_data = self._get_default_historical_priors()
        
        # Audit results by layers
        layer1_results = self._audit_direct_evidence(nodes, scm_dag, historical_data)
        layer2_results = self._audit_causal_implications(nodes, graph, layer1_results)
        layer3_results = self._audit_systemic_risk(nodes, graph, layer1_results, layer2_results, pdet_alignment)
        
        # Generate optimal remediation recommendations
        recommendations = self._generate_optimal_remediations(
            layer1_results, layer2_results, layer3_results
        )
        
        audit_report = {
            'direct_evidence': layer1_results,
            'causal_implications': layer2_results,
            'systemic_risk': layer3_results,
            'recommendations': recommendations,
            'summary': {
                'total_nodes': len(nodes),
                'critical_omissions': sum(1 for r in layer1_results.values() 
                                        if r.get('omission_severity') == 'critical'),
                'expected_success_probability': layer3_results.get('success_probability', 0.0),
                'risk_score': layer3_results.get('risk_score', 0.0)
            }
        }
        
        self.logger.info(f"AuditorÃ­a contrafactual completada: "
                        f"{audit_report['summary']['critical_omissions']} omisiones crÃ­ticas detectadas")
        
        return audit_report
    
    def _build_normative_dag(self) -> nx.DiGraph:
        """Build normative DAG of expected relationships in well-formed plans"""
        dag = nx.DiGraph()
        
        # Define normative structure
        # Each goal type should have these attributes
        dag.add_node('baseline', type='required_attribute')
        dag.add_node('target', type='required_attribute')
        dag.add_node('entity', type='required_attribute')
        dag.add_node('budget', type='recommended_attribute')
        dag.add_node('mechanism', type='recommended_attribute')
        dag.add_node('timeline', type='optional_attribute')
        dag.add_node('risk_factors', type='optional_attribute')
        
        # Causal relationships
        dag.add_edge('baseline', 'target', relation='defines_gap')
        dag.add_edge('entity', 'mechanism', relation='executes')
        dag.add_edge('budget', 'mechanism', relation='enables')
        dag.add_edge('mechanism', 'target', relation='achieves')
        dag.add_edge('risk_factors', 'target', relation='threatens')
        
        return dag
    
    def _get_default_historical_priors(self) -> Dict[str, Any]:
        """Get default historical priors if no data is available"""
        return {
            'entity_presence_success_rate': 0.94,
            'baseline_presence_success_rate': 0.89,
            'target_presence_success_rate': 0.92,
            'budget_presence_success_rate': 0.78,
            'mechanism_presence_success_rate': 0.65,
            'complete_documentation_success_rate': 0.82,
            'node_type_success_rates': {
                'producto': 0.85,
                'resultado': 0.72,
                'impacto': 0.58
            }
        }
    
    def _audit_direct_evidence(self, nodes: Dict[str, MetaNode],
                               scm_dag: nx.DiGraph,
                               historical_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        """Layer 1: Audit direct evidence of required components
        
        Enhanced with highly specific Bayesian priors for rare evidence items.
        Example: D2-Q4 risk matrix, D5-Q5 unwanted effects are rare in poor PDMs.
        """
        results = {}
        
        # Load highly specific priors for rare evidence types
        # D2-Q4: Risk matrices are rare in poor PDMs (high probative value as Smoking Gun)
        rare_evidence_priors = {
            'risk_matrix': {
                'prior_alpha': 1.5,  # Low alpha = rare occurrence
                'prior_beta': 12.0,  # High beta = high failure rate when absent
                'keywords': ['matriz de riesgo', 'anÃ¡lisis de riesgo', 'gestiÃ³n de riesgo', 'riesgos identificados']
            },
            'unwanted_effects': {
                'prior_alpha': 1.8,  # D5-Q5: Effects analysis is also rare
                'prior_beta': 10.5,
                'keywords': ['efectos no deseados', 'efectos adversos', 'impactos negativos', 'consecuencias no previstas']
            },
            'theory_of_change': {
                'prior_alpha': 1.2,
                'prior_beta': 15.0,
                'keywords': ['teorÃ­a de cambio', 'teorÃ­a del cambio', 'cadena causal', 'modelo lÃ³gico']
            }
        }
        
        for node_id, node in nodes.items():
            omissions = []
            omission_probs = {}
            rare_evidence_found = {}
            
            # Check for rare, high-value evidence in node text
            node_text_lower = node.text.lower()
            for evidence_type, prior_config in rare_evidence_priors.items():
                if any(kw in node_text_lower for kw in prior_config['keywords']):
                    # Rare evidence found! Strong Smoking Gun
                    rare_evidence_found[evidence_type] = {
                        'prior_alpha': prior_config['prior_alpha'],
                        'prior_beta': prior_config['prior_beta'],
                        'posterior_strength': prior_config['prior_alpha'] / (prior_config['prior_alpha'] + prior_config['prior_beta'])
                    }
                    self.logger.info(f"Rare evidence '{evidence_type}' found in {node_id} - Strong Smoking Gun!")
            
            # Check baseline
            if not node.baseline or str(node.baseline).upper() in ['ND', 'POR DEFINIR', 'N/A', 'NONE']:
                p_failure_given_omission = 1.0 - historical_data.get('baseline_presence_success_rate', 0.89)
                omissions.append('baseline')
                omission_probs['baseline'] = p_failure_given_omission
            
            # Check target
            if not node.target or str(node.target).upper() in ['ND', 'POR DEFINIR', 'N/A', 'NONE']:
                p_failure_given_omission = 1.0 - historical_data.get('target_presence_success_rate', 0.92)
                omissions.append('target')
                omission_probs['target'] = p_failure_given_omission
            
            # Check entity
            if not node.responsible_entity:
                p_failure_given_omission = 1.0 - historical_data.get('entity_presence_success_rate', 0.94)
                omissions.append('entity')
                omission_probs['entity'] = p_failure_given_omission
            
            # Check budget
            if not node.financial_allocation:
                p_failure_given_omission = 1.0 - historical_data.get('budget_presence_success_rate', 0.78)
                omissions.append('budget')
                omission_probs['budget'] = p_failure_given_omission
            
            # Check mechanism
            if not node.entity_activity:
                p_failure_given_omission = 1.0 - historical_data.get('mechanism_presence_success_rate', 0.65)
                omissions.append('mechanism')
                omission_probs['mechanism'] = p_failure_given_omission
            
            # Determine severity
            severity = 'none'
            if omission_probs:
                max_failure_prob = max(omission_probs.values())
                if max_failure_prob > 0.15:
                    severity = 'critical'
                elif max_failure_prob > 0.10:
                    severity = 'high'
                elif max_failure_prob > 0.05:
                    severity = 'medium'
                else:
                    severity = 'low'
            
            results[node_id] = {
                'omissions': omissions,
                'omission_probabilities': omission_probs,
                'omission_severity': severity,
                'node_type': node.type,
                'rare_evidence_found': rare_evidence_found  # Add rare evidence to results
            }
        
        return results
    
    def _audit_causal_implications(self, nodes: Dict[str, MetaNode],
                                   graph: nx.DiGraph,
                                   direct_evidence: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """Layer 2: Audit causal implications of omissions"""
        implications = {}
        
        for node_id, node in nodes.items():
            node_omissions = direct_evidence[node_id]['omissions']
            causal_effects = {}
            
            # If baseline is missing
            if 'baseline' in node_omissions:
                # P(target_miscalibrated | missing_baseline)
                causal_effects['target_miscalibration'] = {
                    'probability': 0.73,
                    'description': 'Sin lÃ­nea base, la meta probablemente estÃ¡ mal calibrada'
                }
            
            # If entity and high budget are missing
            if 'entity' in node_omissions and node.financial_allocation and node.financial_allocation > 1000000:
                causal_effects['implementation_failure'] = {
                    'probability': 0.89,
                    'description': 'Alto presupuesto sin entidad responsable indica alto riesgo de falla'
                }
            elif 'entity' in node_omissions:
                causal_effects['implementation_failure'] = {
                    'probability': 0.65,
                    'description': 'Sin entidad responsable, la implementaciÃ³n es incierta'
                }
            
            # If mechanism is missing
            if 'mechanism' in node_omissions:
                causal_effects['unclear_pathway'] = {
                    'probability': 0.70,
                    'description': 'Sin mecanismo definido, la vÃ­a causal es opaca'
                }
            
            # Check downstream effects
            successors = list(graph.successors(node_id)) if graph.has_node(node_id) else []
            if node_omissions and successors:
                causal_effects['cascade_risk'] = {
                    'probability': min(0.95, 0.4 + 0.1 * len(node_omissions)),
                    'affected_nodes': successors,
                    'description': f'Omisiones pueden afectar {len(successors)} nodos dependientes'
                }
            
            implications[node_id] = {
                'causal_effects': causal_effects,
                'total_risk': sum(e['probability'] for e in causal_effects.values()) / max(len(causal_effects), 1)
            }
        
        return implications
    
    def _audit_systemic_risk(self, nodes: Dict[str, MetaNode],
                            graph: nx.DiGraph,
                            direct_evidence: Dict[str, Dict[str, Any]],
                            causal_implications: Dict[str, Dict[str, Any]],
                            pdet_alignment: Optional[float] = None) -> Dict[str, Any]:
        """
        AUDIT POINT 2.3: Policy Alignment Dual Constraint
        Layer 3: Calculate systemic risk from accumulated omissions
        
        Harmonic Front 3 - Enhancement 1: Alignment and Systemic Risk Linkage
        Incorporates Policy Alignment scores (PND, ODS, RRI) as variable in systemic risk.
        
        For D5-Q4 (Riesgos SistÃ©micos) and D4-Q5 (AlineaciÃ³n):
        - If pdet_alignment â‰¤ 0.60, applies 1.2Ã— multiplier to risk_score
        - Excelente on D5-Q4 requires risk_score < 0.10
        
        Implements dual constraints integrating macro-micro causality per Lieberman 2015.
        """
        
        # Identify critical nodes (high centrality)
        if graph.number_of_nodes() > 0:
            try:
                centrality = nx.betweenness_centrality(graph)
            except:
                centrality = {n: 0.5 for n in graph.nodes()}
        else:
            centrality = {}
        
        # Calculate P(cascade_failure | omission_set)
        critical_omissions = []
        for node_id, evidence in direct_evidence.items():
            if evidence['omission_severity'] in ['critical', 'high']:
                node_centrality = centrality.get(node_id, 0.5)
                critical_omissions.append({
                    'node_id': node_id,
                    'severity': evidence['omission_severity'],
                    'centrality': node_centrality,
                    'omissions': evidence['omissions']
                })
        
        # Calculate systemic risk
        if critical_omissions:
            # Weighted by centrality
            risk_score = sum(
                (1.0 if om['severity'] == 'critical' else 0.7) * (om['centrality'] + 0.1)
                for om in critical_omissions
            ) / len(nodes)
        else:
            risk_score = 0.0
        
        # AUDIT POINT 2.3: Policy Alignment Dual Constraint
        # If pdet_alignment â‰¤ 0.60, apply 1.2Ã— multiplier to risk_score
        # This enforces integration between D4-Q5 (AlineaciÃ³n) and D5-Q4 (Riesgos SistÃ©micos)
        alignment_penalty_applied = False
        alignment_threshold = 0.60
        alignment_multiplier = 1.2
        
        if pdet_alignment is not None and pdet_alignment <= alignment_threshold:
            original_risk = risk_score
            risk_score = risk_score * alignment_multiplier
            alignment_penalty_applied = True
            self.logger.warning(
                f"ALIGNMENT PENALTY (D5-Q4): pdet_alignment={pdet_alignment:.2f} â‰¤ {alignment_threshold}, "
                f"risk_score escalated from {original_risk:.3f} to {risk_score:.3f} "
                f"(multiplier: {alignment_multiplier}Ã—). Dual constraint per Lieberman 2015."
            )
        
        # Calculate P(success | current_state)
        total_omissions = sum(len(e['omissions']) for e in direct_evidence.values())
        total_possible = len(nodes) * 5  # 5 key attributes per node
        completeness = 1.0 - (total_omissions / max(total_possible, 1))
        
        # Success probability (simplified Bayesian)
        base_success_rate = 0.70
        success_probability = base_success_rate * completeness
        
        # D5-Q4 quality criteria check (AUDIT POINT 2.3)
        # Excellent requires risk_score < 0.10 (matching ODS benchmarks per UN 2020)
        d5_q4_quality = 'insuficiente'
        risk_threshold_excellent = 0.10
        risk_threshold_good = 0.20
        risk_threshold_acceptable = 0.35
        
        if risk_score < risk_threshold_excellent:
            d5_q4_quality = 'excelente'
        elif risk_score < risk_threshold_good:
            d5_q4_quality = 'bueno'
        elif risk_score < risk_threshold_acceptable:
            d5_q4_quality = 'aceptable'
        
        # Flag if alignment is causing quality failure
        alignment_causing_failure = (
            alignment_penalty_applied and 
            original_risk < risk_threshold_excellent and 
            risk_score >= risk_threshold_excellent
        )
        
        return {
            'risk_score': min(1.0, risk_score),
            'success_probability': success_probability,
            'critical_omissions': critical_omissions,
            'completeness': completeness,
            'total_omissions': total_omissions,
            'pdet_alignment': pdet_alignment,
            'alignment_penalty_applied': alignment_penalty_applied,
            'alignment_threshold': alignment_threshold,
            'alignment_multiplier': alignment_multiplier,
            'alignment_causing_failure': alignment_causing_failure,
            'd5_q4_quality': d5_q4_quality,
            'd4_q5_alignment_score': pdet_alignment,
            'risk_thresholds': {
                'excellent': risk_threshold_excellent,
                'good': risk_threshold_good,
                'acceptable': risk_threshold_acceptable
            }
        }
    
    def _generate_optimal_remediations(self, 
                                      direct_evidence: Dict[str, Dict[str, Any]],
                                      causal_implications: Dict[str, Dict[str, Any]],
                                      systemic_risk: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate prioritized remediation recommendations"""
        remediations = []
        
        # Calculate expected value of information for each remediation
        for node_id, evidence in direct_evidence.items():
            if not evidence['omissions']:
                continue
            
            for omission in evidence['omissions']:
                # Estimate impact
                omission_prob = evidence['omission_probabilities'].get(omission, 0.1)
                causal_risk = causal_implications[node_id]['total_risk']
                
                # Expected value = P(failure_avoided) * Impact
                expected_value = omission_prob * (1 + causal_risk)
                
                # Effort estimate (simplified)
                effort_map = {
                    'baseline': 3,  # Moderate effort to research
                    'target': 2,    # Low effort to define
                    'entity': 2,    # Low effort to assign
                    'budget': 4,    # Higher effort to allocate
                    'mechanism': 5  # Highest effort to design
                }
                effort = effort_map.get(omission, 3)
                
                # Priority = Expected Value / Effort
                priority = expected_value / effort
                
                remediations.append({
                    'node_id': node_id,
                    'omission': omission,
                    'severity': evidence['omission_severity'],
                    'expected_value': expected_value,
                    'effort': effort,
                    'priority': priority,
                    'recommendation': self._get_remediation_text(omission, node_id)
                })
        
        # Sort by priority (descending)
        remediations.sort(key=lambda x: x['priority'], reverse=True)
        
        return remediations
    
    def _get_remediation_text(self, omission: str, node_id: str) -> str:
        """Get specific remediation text for an omission"""
        texts = {
            'baseline': f"Definir lÃ­nea base cuantitativa para {node_id} basada en diagnÃ³stico actual",
            'target': f"Especificar meta cuantitativa alcanzable para {node_id} con horizonte temporal",
            'entity': f"Asignar entidad responsable clara para la ejecuciÃ³n de {node_id}",
            'budget': f"Asignar recursos presupuestarios especÃ­ficos a {node_id}",
            'mechanism': f"Documentar mecanismo causal (Entidad-Actividad) para {node_id}"
        }
        return texts.get(omission, f"Completar {omission} para {node_id}")



class BayesianMechanismInference:
    """
    AGUJA II: El Modelo Generativo de Mecanismos
    Hierarchical Bayesian model for causal mechanism inference
    
    F1.2 ARCHITECTURAL REFACTORING:
    This class now integrates with refactored Bayesian engine components:
    - BayesianPriorBuilder: Construye priors adaptativos (AGUJA I)
    - BayesianSamplingEngine: Ejecuta MCMC sampling (AGUJA II)
    - NecessitySufficiencyTester: Ejecuta Hoop Tests (AGUJA III)
    
    The refactored components provide:
    - Crystal-clear separation of concerns
    - Trivial unit testing
    - Explicit compliance with Fronts B and C
    
    Legacy methods are preserved for backward compatibility.
    """
    
    def __init__(self, config: ConfigLoader, nlp_model: spacy.Language) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.nlp = nlp_model
        
        # F1.2: Initialize refactored Bayesian engine adapter if available
        if REFACTORED_BAYESIAN_AVAILABLE:
            try:
                self.bayesian_adapter = BayesianEngineAdapter(config, nlp_model)
                if self.bayesian_adapter.is_available():
                    self.logger.info("âœ“ Usando motor Bayesiano refactorizado (F1.2)")
                    self._log_refactored_components()
                else:
                    self.bayesian_adapter = None
            except Exception as e:
                self.logger.warning(f"Error inicializando motor refactorizado: {e}")
                self.bayesian_adapter = None
        else:
            self.bayesian_adapter = None
        
        # Load mechanism type hyperpriors from configuration (externalized)
        self.mechanism_type_priors = {
            'administrativo': self.config.get_mechanism_prior('administrativo'),
            'tecnico': self.config.get_mechanism_prior('tecnico'),
            'financiero': self.config.get_mechanism_prior('financiero'),
            'politico': self.config.get_mechanism_prior('politico'),
            'mixto': self.config.get_mechanism_prior('mixto')
        }
        
        # Typical activity sequences by mechanism type
        # These could also be externalized if needed for domain-specific customization
        self.mechanism_sequences = {
            'administrativo': ['planificar', 'coordinar', 'gestionar', 'supervisar'],
            'tecnico': ['diagnosticar', 'diseÃ±ar', 'implementar', 'evaluar'],
            'financiero': ['asignar', 'ejecutar', 'auditar', 'reportar'],
            'politico': ['concertar', 'negociar', 'aprobar', 'promulgar']
        }
        
        # Track inferred mechanisms
        self.inferred_mechanisms: Dict[str, Dict[str, Any]] = {}
    
    def _log_refactored_components(self) -> None:
        """Log status of refactored Bayesian components (F1.2)"""
        if self.bayesian_adapter:
            status = self.bayesian_adapter.get_component_status()
            self.logger.info("  - BayesianPriorBuilder: " + 
                           ("âœ“" if status['prior_builder_ready'] else "âœ—"))
            self.logger.info("  - BayesianSamplingEngine: " + 
                           ("âœ“" if status['sampling_engine_ready'] else "âœ—"))
            self.logger.info("  - NecessitySufficiencyTester: " + 
                           ("âœ“" if status['necessity_tester_ready'] else "âœ—"))
    
    def infer_mechanisms(self, nodes: Dict[str, MetaNode], 
                        text: str) -> Dict[str, Dict[str, Any]]:
        """
        Infer latent causal mechanisms using hierarchical Bayesian modeling
        
        HARMONIC FRONT 4 ENHANCEMENT:
        - Tracks mean mechanism_type uncertainty for quality criteria
        - Reports uncertainty reduction metrics
        """
        self.logger.info("Iniciando inferencia Bayesiana de mecanismos...")
        
        # Focus on 'producto' nodes which should have mechanisms
        product_nodes = {nid: n for nid, n in nodes.items() if n.type == 'producto'}
        
        # Track uncertainties for mean calculation
        mechanism_uncertainties = []
        
        for node_id, node in product_nodes.items():
            mechanism = self._infer_single_mechanism(node, text, nodes)
            self.inferred_mechanisms[node_id] = mechanism
            
            # Track mechanism type uncertainty for quality criteria
            if 'uncertainty' in mechanism:
                mech_type_uncertainty = mechanism['uncertainty'].get('mechanism_type', 1.0)
                mechanism_uncertainties.append(mech_type_uncertainty)
        
        # Calculate mean mechanism uncertainty for Harmonic Front 4 quality criteria
        mean_mech_uncertainty = (
            np.mean(mechanism_uncertainties) if mechanism_uncertainties else 1.0
        )
        
        self.logger.info(f"Mecanismos inferidos: {len(self.inferred_mechanisms)}")
        self.logger.info(f"Mean mechanism_type uncertainty: {mean_mech_uncertainty:.4f}")
        
        # Store for reporting
        self._mean_mechanism_uncertainty = mean_mech_uncertainty
        
        return self.inferred_mechanisms
    
    
    def _infer_single_mechanism(self, node: MetaNode, text: str, 
                                all_nodes: Dict[str, MetaNode]) -> Dict[str, Any]:
        """Infer mechanism for a single product node"""
        # Extract observations from text
        observations = self._extract_observations(node, text)
        
        # Level 3: Sample mechanism type from hyperprior
        mechanism_type_posterior = self._infer_mechanism_type(observations)
        
        # Level 2: Infer activity sequence given mechanism type
        sequence_posterior = self._infer_activity_sequence(
            observations, mechanism_type_posterior
        )
        
        # Level 1: Calculate coherence factor
        coherence_score = self._calculate_coherence_factor(
            node, observations, all_nodes
        )
        
        # Validation tests
        sufficiency = self._test_sufficiency(node, observations)
        necessity = self._test_necessity(node, observations)
        
        # Quantify uncertainty
        uncertainty = self._quantify_uncertainty(
            mechanism_type_posterior, sequence_posterior, coherence_score
        )
        
        # Detect gaps
        gaps = self._detect_gaps(node, observations, uncertainty)
        
        return {
            'mechanism_type': mechanism_type_posterior,
            'activity_sequence': sequence_posterior,
            'coherence_score': coherence_score,
            'sufficiency_test': sufficiency,
            'necessity_test': necessity,
            'uncertainty': uncertainty,
            'gaps': gaps,
            'observations': observations
        }
    
    def _extract_observations(self, node: MetaNode, text: str) -> Dict[str, Any]:
        """Extract textual observations related to the mechanism"""
        # Find node context in text
        node_pattern = re.escape(node.id)
        matches = list(re.finditer(node_pattern, text, re.IGNORECASE))
        
        observations = {
            'entity_activity': None,
            'verbs': [],
            'entities': [],
            'budget': node.financial_allocation,
            'context_snippets': []
        }
        
        if node.entity_activity:
            observations['entity_activity'] = {
                'entity': node.entity_activity.entity,
                'activity': node.entity_activity.activity,
                'verb_lemma': node.entity_activity.verb_lemma
            }
        
        # Extract context around node mentions
        for match in matches[:3]:  # Limit to first 3 occurrences
            start = max(0, match.start() - 300)
            end = min(len(text), match.end() + 300)
            context = text[start:end]
            
            # Process with spaCy
            doc = self.nlp(context)
            
            # Extract verbs
            verbs = [token.lemma_ for token in doc if token.pos_ == 'VERB']
            observations['verbs'].extend(verbs)
            
            # Extract entities
            entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PER']]
            observations['entities'].extend(entities)
            
            observations['context_snippets'].append(context[:200])
        
        return observations
    
    def _infer_mechanism_type(self, observations: Dict[str, Any]) -> Dict[str, float]:
        """Infer mechanism type using Bayesian updating"""
        # Start with hyperprior
        posterior = dict(self.mechanism_type_priors)
        
        # Get Laplace smoothing parameter from configuration
        laplace_smooth = self.config.get_bayesian_threshold('laplace_smoothing')
        
        # Update based on observed verbs
        observed_verbs = set(observations.get('verbs', []))
        
        if observed_verbs:
            for mech_type, typical_verbs in self.mechanism_sequences.items():
                # Count overlap
                overlap = len(observed_verbs.intersection(set(typical_verbs)))
                total = len(typical_verbs)
                
                if total > 0:
                    # Likelihood: proportion of typical verbs observed with Laplace smoothing
                    likelihood = (overlap + laplace_smooth) / (total + 2 * laplace_smooth)
                    
                    # Bayesian update
                    posterior[mech_type] *= likelihood
        
        # Update based on entity-activity
        if observations.get('entity_activity'):
            verb = observations['entity_activity'].get('verb_lemma', '')
            for mech_type, typical_verbs in self.mechanism_sequences.items():
                if verb in typical_verbs:
                    posterior[mech_type] *= 1.5
        
        # Normalize
        total = sum(posterior.values())
        if total > 0:
            posterior = {k: v / total for k, v in posterior.items()}
        
        return posterior
    
    def _infer_activity_sequence(self, observations: Dict[str, Any],
                                 mechanism_type_posterior: Dict[str, float]) -> Dict[str, Any]:
        """Infer activity sequence parameters"""
        # Get most likely mechanism type
        best_type = max(mechanism_type_posterior.items(), key=lambda x: x[1])[0]
        expected_sequence = self.mechanism_sequences.get(best_type, [])
        
        observed_verbs = observations.get('verbs', [])
        
        # Calculate transition probabilities (simplified Markov chain)
        transitions = {}
        for i in range(len(expected_sequence) - 1):
            current = expected_sequence[i]
            next_verb = expected_sequence[i + 1]
            
            # Check if transition is observed
            if current in observed_verbs and next_verb in observed_verbs:
                transitions[(current, next_verb)] = 0.85
            else:
                transitions[(current, next_verb)] = 0.40
        
        return {
            'expected_sequence': expected_sequence,
            'observed_verbs': observed_verbs,
            'transition_probabilities': transitions,
            'sequence_completeness': len(set(observed_verbs) & set(expected_sequence)) / max(len(expected_sequence), 1)
        }
    
    def _calculate_coherence_factor(self, node: MetaNode, 
                                   observations: Dict[str, Any],
                                   all_nodes: Dict[str, MetaNode]) -> float:
        """Calculate mechanism coherence score"""
        coherence = 0.0
        weights = []
        
        # Factor 1: Entity-Activity presence
        if observations.get('entity_activity'):
            coherence += 0.30
            weights.append(0.30)
        
        # Factor 2: Budget consistency
        if observations.get('budget'):
            coherence += 0.20
            weights.append(0.20)
        
        # Factor 3: Verb sequence completeness
        seq_info = observations.get('verbs', [])
        if seq_info:
            verb_score = min(len(seq_info) / 4.0, 1.0)  # Expect ~4 verbs
            coherence += verb_score * 0.25
            weights.append(0.25)
        
        # Factor 4: Entity presence
        if observations.get('entities'):
            coherence += 0.15
            weights.append(0.15)
        
        # Factor 5: Context richness
        snippets = observations.get('context_snippets', [])
        if snippets:
            coherence += 0.10
            weights.append(0.10)
        
        # Normalize by actual weights used
        if weights:
            coherence = coherence / sum(weights) if sum(weights) > 0 else 0.0
        
        return coherence
    
    def _test_sufficiency(self, node: MetaNode, 
                         observations: Dict[str, Any]) -> Dict[str, Any]:
        """Test if mechanism is sufficient to produce the outcome"""
        # Check if entity has capability
        has_entity = observations.get('entity_activity') is not None
        
        # Check if activities are present
        has_activities = len(observations.get('verbs', [])) >= 2
        
        # Check if resources are allocated
        has_resources = observations.get('budget') is not None
        
        sufficiency_score = (
            (0.4 if has_entity else 0.0) +
            (0.4 if has_activities else 0.0) +
            (0.2 if has_resources else 0.0)
        )
        
        return {
            'score': sufficiency_score,
            'is_sufficient': sufficiency_score >= 0.6,
            'components': {
                'entity': has_entity,
                'activities': has_activities,
                'resources': has_resources
            }
        }
    
    def _test_necessity(self, node: MetaNode, 
                       observations: Dict[str, Any]) -> Dict[str, Any]:
        """
        AUDIT POINT 2.2: Mechanism Necessity Hoop Test
        
        Test if mechanism is necessary by checking documented components:
        - Entity (responsable)
        - Activity (verb lemma sequence)
        - Budget (presupuesto asignado)
        
        Implements Beach 2017 Hoop Tests for necessity verification.
        Per Falleti & Lynch 2009, Bayesian-deterministic hybrid boosts mechanism depth.
        
        Returns:
            Dict with 'is_necessary', 'missing_components', and remediation text
        """
        # F1.2: Use refactored NecessitySufficiencyTester if available
        if self.bayesian_adapter and self.bayesian_adapter.necessity_tester:
            try:
                return self.bayesian_adapter.test_necessity_from_observations(
                    node.id,
                    observations
                )
            except Exception as e:
                self.logger.warning(f"Error en tester refactorizado: {e}, usando legacy")
        
        # AUDIT POINT 2.2: Enhanced necessity test with documented components
        missing_components = []
        
        # 1. Check Entity documentation
        entities = observations.get('entities', [])
        entity_activity = observations.get('entity_activity')
        
        if not entity_activity or not entity_activity.get('entity'):
            missing_components.append('entity')
        else:
            # Verify unique entity (not multiple conflicting entities)
            unique_entity = len(set(entities)) == 1 if entities else False
            if not unique_entity and len(entities) > 1:
                missing_components.append('unique_entity')
        
        # 2. Check Activity documentation (verb lemma sequence)
        verbs = observations.get('verbs', [])
        if not verbs or len(verbs) < 1:
            missing_components.append('activity')
        else:
            # Check for specific action verbs (not just generic ones)
            specific_verbs = [v for v in verbs if v in [
                'implementar', 'ejecutar', 'realizar', 'desarrollar',
                'construir', 'diseÃ±ar', 'planificar', 'coordinar',
                'gestionar', 'supervisar', 'controlar', 'auditar'
            ]]
            if not specific_verbs:
                missing_components.append('specific_activity')
        
        # 3. Check Budget documentation
        budget = observations.get('budget')
        if budget is None or budget <= 0:
            missing_components.append('budget')
        
        # Calculate necessity score
        # All three components must be present for necessity=True
        is_necessary = len(missing_components) == 0
        
        # Calculate partial score for reporting
        max_components = 3  # entity, activity, budget
        present_components = max_components - len([c for c in missing_components if c in ['entity', 'activity', 'budget']])
        necessity_score = present_components / max_components
        
        result = {
            'score': necessity_score,
            'is_necessary': is_necessary,
            'missing_components': missing_components,
            'alternatives_likely': not is_necessary,
            'hoop_test_passed': is_necessary
        }
        
        # Add remediation text if test fails
        if not is_necessary:
            result['remediation'] = self._generate_necessity_remediation(node.id, missing_components)
        
        return result
    
    def _generate_necessity_remediation(self, node_id: str, missing_components: List[str]) -> str:
        """Generate remediation text for failed necessity test"""
        component_descriptions = {
            'entity': 'entidad responsable claramente identificada',
            'unique_entity': 'una Ãºnica entidad responsable (mÃºltiples entidades detectadas)',
            'activity': 'secuencia de actividades documentada',
            'specific_activity': 'actividades especÃ­ficas (no genÃ©ricas)',
            'budget': 'presupuesto asignado y cuantificado'
        }
        
        missing_desc = ', '.join([component_descriptions.get(c, c) for c in missing_components])
        
        return (
            f"Mecanismo para {node_id} falla Hoop Test de necesidad (D6-Q2). "
            f"Componentes faltantes: {missing_desc}. "
            f"Se requiere documentar estos componentes necesarios para validar "
            f"la cadena causal segÃºn Beach 2017."
        )
    
    def _quantify_uncertainty(self, mechanism_type_posterior: Dict[str, float],
                             sequence_posterior: Dict[str, Any],
                             coherence_score: float) -> Dict[str, float]:
        """Quantify epistemic uncertainty"""
        # Entropy of mechanism type distribution
        mech_probs = list(mechanism_type_posterior.values())
        if mech_probs:
            mech_entropy = -sum(p * np.log(p + 1e-10) for p in mech_probs if p > 0)
            max_entropy = np.log(len(mech_probs))
            mech_uncertainty = mech_entropy / max_entropy if max_entropy > 0 else 1.0
        else:
            mech_uncertainty = 1.0
        
        # Sequence completeness uncertainty
        seq_completeness = sequence_posterior.get('sequence_completeness', 0.0)
        seq_uncertainty = 1.0 - seq_completeness
        
        # Coherence uncertainty
        coherence_uncertainty = 1.0 - coherence_score
        
        # Combined uncertainty
        total_uncertainty = (
            mech_uncertainty * 0.4 +
            seq_uncertainty * 0.3 +
            coherence_uncertainty * 0.3
        )
        
        return {
            'total': total_uncertainty,
            'mechanism_type': mech_uncertainty,
            'sequence': seq_uncertainty,
            'coherence': coherence_uncertainty
        }
    
    def _detect_gaps(self, node: MetaNode, observations: Dict[str, Any],
                    uncertainty: Dict[str, float]) -> List[Dict[str, str]]:
        """Detect documentation gaps based on uncertainty"""
        gaps = []
        
        # High total uncertainty
        if uncertainty['total'] > 0.6:
            gaps.append({
                'type': 'high_uncertainty',
                'severity': 'high',
                'message': f"Mecanismo para {node.id} tiene alta incertidumbre ({uncertainty['total']:.2f})",
                'suggestion': "Se requiere mÃ¡s documentaciÃ³n sobre el mecanismo causal"
            })
        
        # Missing entity
        if not observations.get('entity_activity'):
            gaps.append({
                'type': 'missing_entity',
                'severity': 'high',
                'message': f"No se especifica entidad responsable para {node.id}",
                'suggestion': "Especificar quÃ© entidad ejecutarÃ¡ las actividades"
            })
        
        # Insufficient activities
        if len(observations.get('verbs', [])) < 2:
            gaps.append({
                'type': 'insufficient_activities',
                'severity': 'medium',
                'message': f"Pocas actividades documentadas para {node.id}",
                'suggestion': "Detallar las actividades necesarias para lograr el producto"
            })
        
        # Missing budget
        if not observations.get('budget'):
            gaps.append({
                'type': 'missing_budget',
                'severity': 'medium',
                'message': f"Sin asignaciÃ³n presupuestaria para {node.id}",
                'suggestion': "Asignar recursos financieros al producto"
            })
        
        return gaps


class CausalInferenceSetup:
    """Prepare model for causal inference"""
    
    def __init__(self, config: ConfigLoader) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.goal_classification = config.get('lexicons.goal_classification', {})
        self.admin_keywords = config.get('lexicons.administrative_keywords', [])
        self.contextual_factors = config.get('lexicons.contextual_factors', [])
    
    def classify_goal_dynamics(self, nodes: Dict[str, MetaNode]) -> None:
        """Classify dynamics for each goal"""
        for node in nodes.values():
            text_lower = node.text.lower()
            
            for keyword, dynamics in self.goal_classification.items():
                if keyword in text_lower:
                    node.dynamics = cast(DynamicsType, dynamics)
                    self.logger.debug(f"Meta {node.id} clasificada como {node.dynamics}")
                    break
    
    def assign_probative_value(self, nodes: Dict[str, MetaNode]) -> None:
        """Assign probative test types to nodes"""
        # Import INDICATOR_STRUCTURE from financiero_viabilidad_tablas
        try:
            from financiero_viabilidad_tablas import ColombianMunicipalContext
            indicator_structure = ColombianMunicipalContext.INDICATOR_STRUCTURE
        except ImportError:
            indicator_structure = {
                'resultado': ['lÃ­nea_base', 'meta', 'aÃ±o_base', 'aÃ±o_meta', 'fuente', 'responsable'],
                'producto': ['indicador', 'fÃ³rmula', 'unidad_medida', 'lÃ­nea_base', 'meta', 'periodicidad'],
                'gestiÃ³n': ['eficacia', 'eficiencia', 'economÃ­a', 'costo_beneficio']
            }
        
        for node in nodes.values():
            text_lower = node.text.lower()
            
            # Cross-reference with INDICATOR_STRUCTURE to classify critical requirements
            # as Hoop Tests or Smoking Guns
            required_fields = indicator_structure.get(node.type, [])
            
            # Check if node has all critical DNP requirements (D3-Q1 indicators)
            has_linea_base = bool(node.baseline and str(node.baseline).upper() not in ['ND', 'POR DEFINIR', 'N/A', 'NONE'])
            has_meta = bool(node.target and str(node.target).upper() not in ['ND', 'POR DEFINIR', 'N/A', 'NONE'])
            has_fuente = 'fuente' in text_lower or 'fuente de informaciÃ³n' in text_lower
            
            # Perfect Hoop Test: Missing any critical requirement = total hypothesis failure
            # This applies to producto nodes with D3-Q1 indicators
            if node.type == 'producto':
                if has_linea_base and has_meta and has_fuente:
                    # Perfect indicators trigger Hoop Test classification
                    node.test_type = 'hoop_test'
                    self.logger.debug(f"Meta {node.id} classified as hoop_test (perfect D3-Q1 compliance)")
                elif not has_linea_base or not has_meta:
                    # Missing critical requirements - still Hoop Test but will fail
                    node.test_type = 'hoop_test'
                    node.audit_flags.append('hoop_test_failure')
                    self.logger.warning(f"Meta {node.id} FAILS hoop_test (missing D3-Q1 critical fields)")
                else:
                    node.test_type = 'straw_in_wind'
            # Check for administrative/regulatory nature (Hoop Test)
            elif any(keyword in text_lower for keyword in self.admin_keywords):
                node.test_type = 'hoop_test'
            # Check for highly specific outcomes (Smoking Gun)
            elif node.type == 'resultado' and node.target and node.baseline:
                try:
                    float(str(node.target).replace(',', '').replace('%', ''))
                    # Smoking Gun: rare, highly specific evidence with strong inferential power
                    node.test_type = 'smoking_gun'
                except (ValueError, TypeError):
                    node.test_type = 'straw_in_wind'
            # Double decisive for critical impact goals
            elif node.type == 'impacto' and node.rigor_status == 'fuerte':
                node.test_type = 'doubly_decisive'
            else:
                node.test_type = 'straw_in_wind'
            
            self.logger.debug(f"Meta {node.id} asignada test type: {node.test_type}")
    
    def identify_failure_points(self, graph: nx.DiGraph, text: str) -> Set[str]:
        """Identify single points of failure in causal chain
        
        Harmonic Front 3 - Enhancement 2: Contextual Failure Point Detection
        Expands risk_pattern to explicitly include localized contextual factors from rubrics:
        - restricciones territoriales
        - patrones culturales machistas
        - limitaciÃ³n normativa
        
        For D6-Q5 (Enfoque Diferencial/Restricciones): Excelente requires â‰¥3 distinct
        contextual factors correctly mapped to nodes, satisfying enfoque_diferencial
        and analisis_contextual criteria.
        """
        failure_points = set()
        
        # Find nodes with high out-degree (many dependencies)
        for node_id in graph.nodes():
            out_degree = graph.out_degree(node_id)
            node_type = graph.nodes[node_id].get('type')
            
            if node_type == 'producto' and out_degree >= 3:
                failure_points.add(node_id)
                self.logger.warning(f"Punto Ãºnico de falla identificado: {node_id} "
                                  f"(grado de salida: {out_degree})")
        
        # HARMONIC FRONT 3 - Enhancement 2: Expand contextual factors
        # Add specific rubric factors for D6-Q5 compliance
        extended_contextual_factors = list(self.contextual_factors) + [
            'restricciones territoriales',
            'restricciÃ³n territorial', 
            'limitaciÃ³n territorial',
            'patrones culturales machistas',
            'machismo',
            'inequidad de gÃ©nero',
            'violencia de gÃ©nero',
            'limitaciÃ³n normativa',
            'limitaciÃ³n legal',
            'restricciÃ³n legal',
            'barrera institucional',
            'restricciÃ³n presupuestal',
            'ausencia de capacidad tÃ©cnica',
            'baja capacidad institucional',
            'conflicto armado',
            'desplazamiento forzado',
            'poblaciÃ³n dispersa',
            'ruralidad dispersa',
            'acceso vial limitado',
            'conectividad deficiente'
        ]
        
        # Extract contextual risks from text
        risk_pattern = '|'.join(re.escape(factor) for factor in extended_contextual_factors)
        risk_regex = re.compile(rf'\b({risk_pattern})\b', re.IGNORECASE)
        
        # Track distinct contextual factors for D6-Q5 quality criteria
        contextual_factors_detected = set()
        node_contextual_map = defaultdict(set)
        
        # Find risk mentions and associate with nodes
        for match in risk_regex.finditer(text):
            risk_text = match.group()
            contextual_factors_detected.add(risk_text.lower())
            
            context_start = max(0, match.start() - 200)
            context_end = min(len(text), match.end() + 200)
            context = text[context_start:context_end]
            
            # Try to find node mentions in risk context
            for node_id in graph.nodes():
                if node_id in context:
                    failure_points.add(node_id)
                    if 'contextual_risks' not in graph.nodes[node_id]:
                        graph.nodes[node_id]['contextual_risks'] = []
                    graph.nodes[node_id]['contextual_risks'].append(risk_text)
                    node_contextual_map[node_id].add(risk_text.lower())
        
        # D6-Q5 quality criteria assessment
        distinct_factors_count = len(contextual_factors_detected)
        d6_q5_quality = 'insuficiente'
        if distinct_factors_count >= 3:
            d6_q5_quality = 'excelente'
        elif distinct_factors_count >= 2:
            d6_q5_quality = 'bueno'
        elif distinct_factors_count >= 1:
            d6_q5_quality = 'aceptable'
        
        # Store D6-Q5 metrics in graph attributes
        graph.graph['d6_q5_contextual_factors'] = list(contextual_factors_detected)
        graph.graph['d6_q5_distinct_count'] = distinct_factors_count
        graph.graph['d6_q5_quality'] = d6_q5_quality
        graph.graph['d6_q5_node_mapping'] = dict(node_contextual_map)
        
        self.logger.info(f"Puntos de falla identificados: {len(failure_points)}")
        self.logger.info(f"D6-Q5: {distinct_factors_count} factores contextuales distintos detectados - {d6_q5_quality}")
        
        return failure_points


class ReportingEngine:
    """Generate visualizations and reports"""
    
    def __init__(self, config: ConfigLoader, output_dir: Path) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_causal_diagram(self, graph: nx.DiGraph, policy_code: str) -> Path:
        """Generate causal diagram visualization"""
        dot = Dot(graph_type='digraph', rankdir='TB')
        dot.set_name(f'{policy_code}_causal_model')
        dot.set_node_defaults(
            shape='box',
            style='rounded,filled',
            fontname='Arial',
            fontsize='10'
        )
        dot.set_edge_defaults(
            fontsize='8',
            fontname='Arial'
        )
        
        # Add nodes with rigor coloring
        for node_id in graph.nodes():
            node_data = graph.nodes[node_id]
            
            # Determine color based on rigor status and audit flags
            rigor = node_data.get('rigor_status', 'sin_evaluar')
            audit_flags = node_data.get('audit_flags', [])
            financial = node_data.get('financial_allocation')
            
            if rigor == 'dÃ©bil' or not financial:
                color = 'lightcoral'  # Red
            elif audit_flags:
                color = 'lightyellow'  # Yellow
            else:
                color = 'lightgreen'  # Green
            
            # Create label
            node_type = node_data.get('type', 'programa')
            text = node_data.get('text', '')[:80]
            label = f"{node_id}\\n[{node_type.upper()}]\\n{text}..."
            
            entity = node_data.get('responsible_entity')
            if entity:
                label += f"\\nðŸ‘¤ {entity[:30]}"
            
            if financial:
                label += f"\\nðŸ’° ${financial:,.0f}"
            
            dot_node = Node(
                node_id,
                label=label,
                fillcolor=color
            )
            dot.add_node(dot_node)
        
        # Add edges with causal logic
        for source, target in graph.edges():
            edge_data = graph.edges[source, target]
            keyword = edge_data.get('keyword', '')
            strength = edge_data.get('strength', 0.5)
            
            # Determine edge style based on strength
            style = 'solid' if strength > 0.7 else 'dashed'
            
            dot_edge = Edge(
                source,
                target,
                label=keyword[:20],
                style=style
            )
            dot.add_edge(dot_edge)
        
        # Save files
        dot_path = self.output_dir / f"{policy_code}_causal_diagram.dot"
        png_path = self.output_dir / f"{policy_code}_causal_diagram.png"
        
        try:
            with open(dot_path, 'w', encoding='utf-8') as f:
                f.write(dot.to_string())
            self.logger.info(f"Diagrama DOT guardado en: {dot_path}")
            
            # Try to render PNG
            try:
                dot.write_png(str(png_path))
                self.logger.info(f"Diagrama PNG renderizado en: {png_path}")
            except Exception as e:
                self.logger.warning(f"No se pudo renderizar PNG (Â¿Graphviz instalado?): {e}")
        except Exception as e:
            self.logger.error(f"Error guardando diagrama: {e}")
        
        return png_path
    
    def generate_accountability_matrix(self, graph: nx.DiGraph, 
                                      policy_code: str) -> Path:
        """Generate accountability matrix in Markdown"""
        md_path = self.output_dir / f"{policy_code}_accountability_matrix.md"
        
        # Group by impact goals
        impact_goals = [n for n in graph.nodes() 
                       if graph.nodes[n].get('type') == 'impacto']
        
        content = [f"# Matriz de Responsabilidades - {policy_code}\n"]
        content.append(f"*Generado automÃ¡ticamente por CDAF v2.0*\n")
        content.append("---\n\n")
        
        for impact in impact_goals:
            impact_data = graph.nodes[impact]
            content.append(f"## Meta de Impacto: {impact}\n")
            content.append(f"**DescripciÃ³n:** {impact_data.get('text', 'N/A')}\n\n")
            
            # Find all predecessor chains
            predecessors = list(nx.ancestors(graph, impact))
            
            if predecessors:
                content.append("| Meta | Tipo | Entidad Responsable | Actividad Clave | Presupuesto |\n")
                content.append("|------|------|---------------------|-----------------|-------------|\n")
                
                for pred in predecessors:
                    pred_data = graph.nodes[pred]
                    meta_type = pred_data.get('type', 'N/A')
                    entity = pred_data.get('responsible_entity', 'No asignado')
                    
                    ea = pred_data.get('entity_activity')
                    activity = 'N/A'
                    if ea and isinstance(ea, dict):
                        activity = ea.get('activity', 'N/A')
                    
                    budget = pred_data.get('financial_allocation')
                    budget_str = f"${budget:,.0f}" if budget else "Sin presupuesto"
                    
                    content.append(f"| {pred} | {meta_type} | {entity} | {activity} | {budget_str} |\n")
                
                content.append("\n")
            else:
                content.append("*No se encontraron metas intermedias.*\n\n")
        
        content.append("\n---\n")
        content.append("### Leyenda\n")
        content.append("- **Meta de Impacto:** Resultado final esperado\n")
        content.append("- **Meta de Resultado:** Cambio intermedio observable\n")
        content.append("- **Meta de Producto:** Entrega tangible del programa\n")
        
        try:
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(''.join(content))
            self.logger.info(f"Matriz de responsabilidades guardada en: {md_path}")
        except Exception as e:
            self.logger.error(f"Error guardando matriz de responsabilidades: {e}")
        
        return md_path
    
    def generate_confidence_report(self, 
                                   nodes: Dict[str, MetaNode],
                                   graph: nx.DiGraph,
                                   causal_chains: List[CausalLink],
                                   audit_results: Dict[str, AuditResult],
                                   financial_auditor: FinancialAuditor,
                                   sequence_warnings: List[str],
                                   policy_code: str) -> Path:
        """Generate extraction confidence report"""
        json_path = self.output_dir / f"{policy_code}_extraction_confidence_report.json"
        
        # Calculate metrics
        total_metas = len(nodes)
        
        metas_with_ea = sum(1 for n in nodes.values() if n.entity_activity)
        metas_with_ea_pct = (metas_with_ea / total_metas * 100) if total_metas > 0 else 0
        
        enlaces_with_logic = sum(1 for link in causal_chains if link.get('logic'))
        total_edges = graph.number_of_edges()
        enlaces_with_logic_pct = (enlaces_with_logic / total_edges * 100) if total_edges > 0 else 0
        
        metas_passed_audit = sum(1 for r in audit_results.values() if r['passed'])
        metas_with_traceability_pct = (metas_passed_audit / total_metas * 100) if total_metas > 0 else 0
        
        metas_with_financial = sum(1 for n in nodes.values() if n.financial_allocation)
        metas_with_financial_pct = (metas_with_financial / total_metas * 100) if total_metas > 0 else 0
        
        # Node type distribution
        type_distribution = defaultdict(int)
        for node in nodes.values():
            type_distribution[node.type] += 1
        
        # Rigor distribution
        rigor_distribution = defaultdict(int)
        for node in nodes.values():
            rigor_distribution[node.rigor_status] += 1
        
        report = {
            "metadata": {
                "policy_code": policy_code,
                "framework_version": "2.0.0",
                "total_nodes": total_metas,
                "total_edges": total_edges
            },
            "extraction_metrics": {
                "total_metas_identificadas": total_metas,
                "metas_con_EA_extraido": metas_with_ea,
                "metas_con_EA_extraido_pct": round(metas_with_ea_pct, 2),
                "enlaces_con_logica_causal": enlaces_with_logic,
                "enlaces_con_logica_causal_pct": round(enlaces_with_logic_pct, 2),
                "metas_con_trazabilidad_evidencia": metas_passed_audit,
                "metas_con_trazabilidad_evidencia_pct": round(metas_with_traceability_pct, 2),
                "metas_con_trazabilidad_financiera": metas_with_financial,
                "metas_con_trazabilidad_financiera_pct": round(metas_with_financial_pct, 2)
            },
            "financial_audit": {
                "tablas_financieras_parseadas_exitosamente": financial_auditor.successful_parses,
                "tablas_financieras_fallidas": financial_auditor.failed_parses,
                "asignaciones_presupuestarias_rastreadas": len(financial_auditor.financial_data)
            },
            "sequence_audit": {
                "alertas_secuencia_logica": len(sequence_warnings),
                "detalles": sequence_warnings
            },
            "type_distribution": dict(type_distribution),
            "rigor_distribution": dict(rigor_distribution),
            "audit_summary": {
                "total_audited": len(audit_results),
                "passed": sum(1 for r in audit_results.values() if r['passed']),
                "failed": sum(1 for r in audit_results.values() if not r['passed']),
                "total_warnings": sum(len(r['warnings']) for r in audit_results.values()),
                "total_errors": sum(len(r['errors']) for r in audit_results.values())
            },
            "quality_score": self._calculate_quality_score(
                metas_with_traceability_pct,
                metas_with_financial_pct,
                enlaces_with_logic_pct,
                metas_with_ea_pct
            )
        }
        
        try:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Reporte de confianza guardado en: {json_path}")
        except Exception as e:
            self.logger.error(f"Error guardando reporte de confianza: {e}")
        
        return json_path
    
    def _calculate_quality_score(self, traceability: float, financial: float, 
                                 logic: float, ea: float) -> float:
        """Calculate overall quality score (0-100)"""
        weights = {'traceability': 0.35, 'financial': 0.25, 'logic': 0.25, 'ea': 0.15}
        score = (traceability * weights['traceability'] +
                financial * weights['financial'] +
                logic * weights['logic'] +
                ea * weights['ea'])
        return round(score, 2)
    
    def generate_causal_model_json(self, graph: nx.DiGraph, nodes: Dict[str, MetaNode],
                                  policy_code: str) -> Path:
        """Generate structured JSON export of causal model"""
        json_path = self.output_dir / f"{policy_code}_causal_model.json"
        
        # Prepare node data
        nodes_data = {}
        for node_id, node in nodes.items():
            node_dict = asdict(node)
            # Convert NamedTuple to dict
            if node.entity_activity:
                node_dict['entity_activity'] = node.entity_activity._asdict()
            nodes_data[node_id] = node_dict
        
        # Prepare edge data
        edges_data = []
        for source, target in graph.edges():
            edge_dict = {
                'source': source,
                'target': target,
                **graph.edges[source, target]
            }
            edges_data.append(edge_dict)
        
        model_data = {
            "policy_code": policy_code,
            "framework_version": "2.0.0",
            "nodes": nodes_data,
            "edges": edges_data,
            "statistics": {
                "total_nodes": len(nodes_data),
                "total_edges": len(edges_data),
                "node_types": {
                    node_type: sum(1 for n in nodes.values() if n.type == node_type)
                    for node_type in ['programa', 'producto', 'resultado', 'impacto']
                }
            }
        }
        
        try:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(model_data, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Modelo causal JSON guardado en: {json_path}")
        except Exception as e:
            self.logger.error(f"Error guardando modelo causal: {e}")
        
        return json_path


class CDAFFramework:
    """Main orchestrator for the CDAF pipeline"""
    
    def __init__(self, config_path: Path, output_dir: Path, log_level: str = "INFO") -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.setLevel(getattr(logging, log_level.upper()))
        
        # Initialize components
        self.config = ConfigLoader(config_path)
        self.output_dir = output_dir
        
        # Initialize retry handler for external dependencies
        try:
            from retry_handler import get_retry_handler, DependencyType
            self.retry_handler = get_retry_handler()
            retry_enabled = True
        except ImportError:
            self.logger.warning("RetryHandler no disponible, funcionando sin retry logic")
            self.retry_handler = None
            retry_enabled = False
        
        # Load spaCy model with retry logic
        if retry_enabled and self.retry_handler:
            @self.retry_handler.with_retry(
                DependencyType.SPACY_MODEL,
                operation_name="load_spacy_model",
                exceptions=(OSError, IOError, ImportError)
            )
            def load_spacy_with_retry():
                try:
                    nlp = spacy.load("es_core_news_lg")
                    self.logger.info("Modelo spaCy cargado: es_core_news_lg")
                    return nlp
                except OSError:
                    self.logger.warning("Modelo es_core_news_lg no encontrado. Intentando es_core_news_sm...")
                    nlp = spacy.load("es_core_news_sm")
                    return nlp
            
            try:
                self.nlp = load_spacy_with_retry()
            except OSError:
                self.logger.error("No se encontrÃ³ ningÃºn modelo de spaCy en espaÃ±ol. "
                                "Ejecute: python -m spacy download es_core_news_lg")
                sys.exit(1)
        else:
            # Fallback to original logic without retry
            try:
                self.nlp = spacy.load("es_core_news_lg")
                self.logger.info("Modelo spaCy cargado: es_core_news_lg")
            except OSError:
                self.logger.warning("Modelo es_core_news_lg no encontrado. Intentando es_core_news_sm...")
                try:
                    self.nlp = spacy.load("es_core_news_sm")
                except OSError:
                    self.logger.error("No se encontrÃ³ ningÃºn modelo de spaCy en espaÃ±ol. "
                                    "Ejecute: python -m spacy download es_core_news_lg")
                    sys.exit(1)
        
        # Initialize modules (pass retry_handler to PDF processor)
        self.pdf_processor = PDFProcessor(self.config, retry_handler=self.retry_handler if retry_enabled else None)
        self.causal_extractor = CausalExtractor(self.config, self.nlp)
        self.mechanism_extractor = MechanismPartExtractor(self.config, self.nlp)
        self.bayesian_mechanism = BayesianMechanismInference(self.config, self.nlp)
        self.financial_auditor = FinancialAuditor(self.config)
        self.op_auditor = OperationalizationAuditor(self.config)
        self.inference_setup = CausalInferenceSetup(self.config)
        self.reporting_engine = ReportingEngine(self.config, output_dir)
        
        # Initialize DNP validator if available
        self.dnp_validator = None
        if DNP_AVAILABLE:
            self.dnp_validator = ValidadorDNP(es_municipio_pdet=False)  # Can be configured
            self.logger.info("Validador DNP inicializado")
    
    def process_document(self, pdf_path: Path, policy_code: str) -> bool:
        """Main processing pipeline"""
        self.logger.info(f"Iniciando procesamiento de documento: {pdf_path}")
        
        try:
            # Step 1: Load and extract PDF
            if not self.pdf_processor.load_document(pdf_path):
                return False
